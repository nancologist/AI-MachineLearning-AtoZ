# Machine Learning A-Z: Hands-On Python & R In Data Science

Install packages: `pip3 install $pkg && pip3 freeze > requirements.txt`

# 0. Welcome to the course
## 0.2. Meet your instructors
Hi there,

Hope you are enjoying the course so far!

Not so long ago Hadelin and I did an interview on the SDS Podcast. This is the best place to start if you would like to learn more about his background... and a bit about me too if this is your first course with me :)

Link: 

http://www.superdatascience.com/2

Some of the things you will learn in this podcast:
What is Machine Learning
Mastering Data Science through online courses
What are Recommender Systems
Million dollar question: R vs Python (vs Julia)
What Grand project Hadelin and I are currently working on
Plus you will get an overview of:
Regressions
Classifications
Clustering
Association rule learning
Reinforcement learning
Deep learning


See you in class!

Sincerely,

Kirill Eremenko
___

## 0.3. Learning Path
Hey Data Scientist,

SuperDataScience is bringing you a new learning experience. We know how difficult it is to carve out a career track so we’re introducing the [Machine Learning Skill Track](https://sdsclub.com/learning-paths/machine-learning-track/) to guarantee your way to success.

This Skill Track is a perfect fit if you:
* Struggle to determine the skills you need to succeed in this field,
* Are unsure which courses are right for you,
* Desire to arrange your learning curve efficiently and on your schedule.  

Built to deliver streamlined on-the-job success, the Machine Learning Skill Track provides structured curriculums and discounted courses for in-demand Machine Learning skills.

After completion, Skill Track students will walk away with the required Machine Learning skills and a complete portfolio of work to showcase in competitive job interviews.

Enter this Skill Track to start saving time and money on your Machine Learning journey today.
___

## 0.4. ML vs DL vs AI - What's the difference?
Dear Students,

Kick things off for the course by downloading a helpful cheat sheet: ‘ML vs DL vs AI — What’s the Difference?’ This download addresses one of the most popular questions we hear from students, and hopefully, it clarifies a few lingering questions for you too:

/articles/OU2-Difference-Between-ML-DL-AI.pdf

Enjoy ML!
___

## 0.5. Regression Types
Dear Students,

Watch a great educational video that speaks about the difference between simple linear regression, multiple linear regression, and polynomial regression. Save this resource, or come back and revisit this lecture once you start working on some of the regression models in the course. Please find the video [here](https://www.dropbox.com/s/py3ns8ltraoexfi/Youtube%20-%20Regression%20-%20Video%20Exp%20%232.mov?dl=0).

Enjoy ML!
___

## 0.6. Why Machin Learning is the future?
* For the Dawn of Time until 2005 the human race have created 130 EXABYTES!
* until 2010 that number was : 1,200 EXABYTES
* until 2015 : 7,900 EXABYTES
* estimated for 2020 : 40,900 EXABYTES

Maching Learning can help to use this huge Data more and better.
____

## 0.7. Important notes, tips & tricks for this course
Dear students,

We want you to have the best learning experience during the whole journey of this course. Therefore, please find just below some important notes to understand, as well as tips & tricks to take this course in the best conditions:

1. The practical activities of this course will be done in both Python and R. However, it is absolutely not required to do these practical cases in both programming languages. This course was designed so that people can learn Machine Learning whether they use Python or R in their company, their own business or any specific project. Hence everybody can get comfortable applying Machine Learning on their favorite programming language. But definitely you are not supposed to learn the two. You can do it if you want or if you need it for your work, but usually one of them is sufficient.

2. If you wish to learn both programming languages, just be prepared that there are going to be some repetitions. This is normal, the reasons for this is that there are a lot of similarities between Python and R and of course the practical cases solved in this course are the same for these two programming languages. However, just bear in mind that repetitions are not that bad: you will learn a lot of concepts and techniques in this course and the fact that we repeat the same elements from one programming language to another will help these concepts stick better into your head.

3. The video lectures of this course were recorded at a certain pace so that everybody can adjust the speed to learn at their own rhythm. To adjust the speed you simply need to click the following "Speed" button at the bottom left corner of the video lecture:


    If you find the pace too slow you can increase the speed to 1.25x, 1.5x or even 2x.  
    If you find the pace too fast you can decrease the speed to 0.75x or 0.5x.

4. If there is anything unclear or if you have any question during a Lecture, please keep in mind that there is a very high chance another student asked for some clarifications on the exact same element you need. You can easily find the answer to your question by browsing the Q&A of the specific Lecture you are watching. To browse the Q&A of the Lecture you simply need to click "Browse Q&A" at the bottom of the Lecture:Once you click "Browse Q&A" you will see all the questions and answers related to this Lecture on the right side of the screen:As you can see the questions have titles so you will easily find the same question you are wondering about. If you don't find your question you can of course ask a new question in the Q&A of the Lecture you are in.

We will add more tips & tricks based on our observations of students feedbacks. In the meantime we wish you the best learning experience.

Enjoy Machine Learning!

Kirill & Hadelin
___

## 0.8. This PDF resource will hepl you a lot!
Dear students,

We just made this Latex [PDF](/articles/Machine_Learning_A_Z_Q_A) for you which will considerably help you during your journey in this course. It contains in the first pages the whole structure of the course for you to visualize it clearly, and then 50 pages containing the answers to all the most frequently asked and most important questions, section by section.

In the Table of contents, the sections of the courses are hyperlinked, so you can very easily navigate to the section you want. For example, let's say you are in Part 2 - Regression, in the Multiple Linear Regression section, and you have a question on the Intuition Lectures. Well you just need to go to the Table of contents in the PDF, then go to Part 2 and then click "2.2.1 Multiple Linear Regression Intuition". And this will redirect you to the section you are looking for in the PDF. You may not find your question exactly, but the questions and answers that you will find will definitely bring you more clarity.

This PDF was written in Latex, which means it is super clean. We provided cleaned code snippets, nice charts, and all the mathematical equations to explain some concepts are written clearly.

This PDF will constantly be updated. As soon as we see new important and relevant questions that are repeatedly asked, we will add them in the PDF. Just make sure to re-download it in the course from time to time.

We hope this will help you and make you stronger in Machine Learning, keep up the great work!

Kirill & Hadelin

[PDF](/articles/Machine_Learning_A_Z_Q_A)
___

## 0.9. GET ALL THE CODES AND DATASETS HERE:
Hello my friends,

Please find below the link to the folder containing all the Python codes, R codes, and datasets of this course:

https://drive.google.com/drive/folders/1OFNnrHRZPZ3unWdErjLHod8Ibv2FfG1d?usp=sharing

This folder contains the Python files in .ipynb format, which is the format used to code in Python on Jupyter Notebook or Google Colaboratory.

__Important Note 1:__ Please download this folder on your machine in order to get all the files, especially the datasets which we will have to upload when training the Machine Learning models. In the practical tutorials you will be guided step by step on how to navigate this folder and start coding in Python & R.

__Important Note 2:__ (Python coders only): In order to open the Python files of this folder with Google Colaboratory, you need to have a Gmail account and sign in to that account. This will automatically open this folder on your Google Drive, which will indeed allow you to open the Python files with Google Colaboratory. Then once you open the file, it will be in read-only mode, so in order to code inside you just have to go to File, and then click "Save a copy in Drive...". This will create a copy of this file on your drive, inside which you will be able to code in Python.

Important Note 3 (Python coders only): If you don't want to code on Google Colaboratory and prefer to code on another Python IDE like Jupyter Notebook or Spyder in Anaconda, you can find all the .ipynb files and the .py files in the folder attached at the bottom of this article.

I can't wait to see you in the practical lectures.

Until then, enjoy Machine Learning!

Hadelin
___


## 0.10. Presentation of the ML A-Z folder, Colaboratory, Jupyter Notebook and Spyder
empty
___

## 0.11. Installing R and R Studio
1. https://cran.r-project.org/

2. Download and Install R

3. www.rstudio.com (IDE for R) (For PyCharm: https://www.jetbrains.com/help/pycharm/r-plugin-support.html#get-started)

4. Download and install RStudio
___

## 0.12. Some Additional Resources
Hey Data Scientist,

Congrats on enrolling in the Machine Learning A-Z course!

In order to ease in to this amazing field, we've selected a great episode you can listen to on your commute, at breakfast or wherever. 

Click here to get started: https://www.superdatascience.com/sds-041-inspiring-journey-totally-different-background-data-science/

Enjoy!
___

## 0.13. FAQBot!
Hello Students!

As an additional resource, we are working on deploying our FAQBot (Mango) to help answer any frequently asked - FAQ related questions for the course. You will see Mango (our bot) in the Q&A to help provide access to FAQ based questions and information quickly.

If you have a chance, please feel free test it out using the link mentioned below as we continue training and testing. To be clear, this bot is in early-stage development. Please be aware of any inaccuracy or error while we continue to improve it.

*Mango is still undergoing training, testing, and revisions.

Hi everyone and welcome to our new chatbot! We are hoping to use this chatbot to help answer some of your questions related to the course, or any general course related information to help continue learning. When interacting with the chatbot, please remember that this is a model in training and we expect to have quite a few iterations, development and testing phases to help it learn. Also, please be as clear as possible when asking questions with Mango.

You can ask questions such as:

How can I obtain the certificate of completion?

How to create a virtual environment?

What is the best choice for a programming language in ML or AI?

How to choose the number of hidden layers in a Neural Network?

Can you explain logistic regression?

Can you explain a KNN?

Should I use Python, R or both?

Where can I get the files for ML A-Z?



But please feel free to ask other course related questions so that Mango can learn. *Please keep them course related.

In addition, some other great resources for further information or assistance to help debug any error include the recommended readings, http://stackoverflow.com/, and https://datascience.stackexchange.com/. Our TA’s will do their best to help answer any questions but as DS/ML/AI engineers, becoming comfortable debugging, searching resources and documentation online will become a critical skill for success.

Lastly, please be aware of any errors, or unexpected outcomes. As Data Scientists we consistently need to test, develop and improve our models. We also want to thank you for helping to test our bot, and we hope it helps answer any questions that you might have!

To test Mango directly:

Mango:

https://www.superdatascience.com/pages/welcome-to-faqbot
___

## 0.14. Your Shortcut To Becoming A Better Data Scientist!
Hi Students!

Did you know that by participating in the Q&A it can help:

Make connections with peers within the domain

Reinforce course information

Gain useful insight related to course content

Lead to more course content completed

Improve and obtain better results from specific models

Have your model or customization featured in the course

One main way to help become a better Data Scientist is by joining the discussions. For this, we want to mention the importance of our community of students here. We continuously see and have had students post helpful information, impressive updates and customizations to algorithms that improved results, and have witnessed connections continuously being made in the Q&A.

We have even featured some students' contributions in the course itself! Imagine being able to mention in an interview, or with an employer that your model was featured in a top course in the Data Science or Artificial Intelligence domain?

This leads us to our main point of adding this which is to help emphasize the importance of self learning for this domain. We can easily show you the benefits of helping others (for those of you who are interested, please see here: https://www.mentalfloss.com/article/71964/7-scientific-benefits-helping-others but, in Machine Learning, Artificial Intelligence, Data Science, and any software or programming related career, it will be a required and critical skill to be able to search and solve problems as they develop. This can include any type of common bugs from programs or issues that may pop up due. As libraries, languages and IDE’s are consistently updated, you may run into a bug every so often. This is just part of the nature of the domain that we are in.

Due to this, we want to highlight the importance of self learning and accessing resources such as Google, StackOverflow (or any of the related StackExchanges), or even discussing questions amongst your fellow peers, is a phenomenal way to learn. For those of you who work in the domain, running into problems and solving them is a core method of learning. Personally, answering and debugging questions in the course has helped me become a better AI Engineer/Data Scientist and developer overall.

In addition, if you would like to practice the information from the course, reinforce concepts, and discuss ideas, discussing and answering questions of your peers is another important method to help learn. This can lead to making connections, study groups and habits, and overall improving your course experience by assisting others. Research has also shown that collaboration and participation can lead to increased completion of the material in the course.

To recap, participating in the Q&A in the course can help you with:

Form study habits

Reinforce course information and gain useful insight

Make connections with peers

Improve your overall course learning experience

Altruism

Purpose

*While participating in the Q&A please abide by Udemy’s Terms and Conditions, and be courteous to all participants. We are all on different stages of learning in the course, and every question is a great opportunity to learn!

We greatly appreciate all of the feedback we continuously receive and keep up the awesome work!
___


# 1. Data Preprocessing
## 1.0. Module Introduction
In each section we first start with Python and then do it with the R.
___

## 1.1. Getting Started
[Data.csv](chapter1_data-preprocessing/py/data/Data.csv) : This file here is like a Retail Company which analysis "Which client purchased one of their products, so these the rows (observations) i this dataset correspond to the different customers of this employee, and their infos... and the last column is about if they bought the Product or not."  

You can see that in this there are some empty cells, which makes this data source more realistic.
___

## 1.2. Importing the Libraries (Python)
For Machine Learning in Python we always need at least these three libraries:

* __Numpy__ : To work with arrays and do mathematical operations
* __matplotlib.pyplot__ : Which allows you to create charts
* __pandas__ : which allows you to import dataset and create easily matrices and vectors.

### Installation
So we should install these libraries which we need into our Virtual Env.:
* ``pip install numpy``
* ``pip install matplotlib``
* ``pip install pandas``
___

## 1.3. Importing Dataset (Python)
Data.csv : This file here is like a Retail Company which analysis "Which client purchased one of their products, so these the rows (observations) i this dataset correspond to the different customers of this employee, and their infos... and the last column is about if they bought the Product or not."

``pandas.read_csv()``: This creates a Data Frame from a .csv file.

### Next Steps
After importing Dataset and store it as a Data Frame we need to do these as the next steps:
1. Creating Matrix of Features.
2. Dependent Variable, a Vector.

### Important Principle in Machine Learning
_IN ANY DATASET WHICH YOU ARE GOING TO TRAIN A MACHINE LEARNING MODEL YOU HAVE THE ENTITIES WHICH ARE THE FEATURES AND THE DEPENDENT VARIABLE (two above steps)._

### Features (Independent Variables) - INPUT VALUES
Features are the columns, which are the independent informations (here: Country, Age, Salary), with them you are going to PREDICT the DEPENDENT VARIABLE (here: "Purchased") [Mori: For the future!].

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html  
``.ilock[ROWS, COLUMNS]`` : To locate indexes  (iloc stands for locate indexes)
``.iloc[:, :-1]`` : This takes all the Rows and also all the columns of the dataset, EXCEPT the last column (which is going to be predicted)

### Dependent Variable - TARGET VALUE (y)
This is usually the last column of the dataset (here: PURCHASED)! Because as you may guess, this company will PREDICT some future customers are going to buy the same product based on these informations.
___

## 1.4. For Python learners, summary of Object-oriented programming: classes & objects
Hello students,

For those of you interested in following the Python tutorials of this course, here is a short summary of what you need to know in Object-oriented programming. In the Python tutorials, I will be talking about classes, objects and methods. Please find below a clear explanation of what these concepts are:

A class is the model of something we want to build. For example, if we make a house construction plan that gathers the instructions on how to build a house, then this construction plan is the class.

An object is an instance of the class. So if we take that same example of the house construction plan, then an object is simply a house. A house (the object) that was built by following the instructions of the construction plan (the class).
And therefore there can be many objects of the same class, because we can build many houses from the construction plan.

A method is a tool we can use on the object to complete a specific action. So in this same example, a tool can be to open the main door of the house if a guest is coming. A method can also be seen as a function that is applied onto the object, takes some inputs (that were defined in the class) and returns some output.

Hope this helps you get the intuition of Object-oriented programming, don't hesitate to ask for more explanations in the Q&A if anything is unclear.

Kind regards,

Hadelin
___

## 1.5. Taking care of Missing Data
There are some missing data, which is normal in Machine Learning! If we look at the dataset we see that the Salary in Row 4 and the Age in the Raw 6 are missing.

You can not leave it like that, because it will cause error by training the model, therefore you must handle them. There are actually several ways to handle them:

1. The 1st way is just to IGNORE those Observations which hav missing data and deleting them. And that would be OK if you have a LARGE dataset so if you have for example 1% Missing data, you know that removing 1% of observations won't change much the LEARNING QUALITY of your MODEL. But sometimes you have a lot of missing data and therefore you must handle them the right way.

2. The 2nd way is to REPLACING the missing data by THE AVERAGE OF ALL THE VALUES IN THAT COLUMN (FEATURE), in which the data is missing.

3. Other ways could be to replace the missing value with the Median of that Column (Feature) or with the maximum frequent value in that column.

### Our Goals
* We want to replace the missing salary by the average of all the salaries, this is a CLASSIC WAY of handling missing data.

### Package ``Scikit Learn``
This is one of the important packages by the Data Preprocessing and it has very good tools for that. We will use it a lot in this course.

``pip install scikit-learn``

### How we handle it?
The class from the package Scikit-Learn is called ``SimpleImputer`` , we are actually going to first import that class, then we will create an Instance of that class, this object will allow us to exactly replace the missing salary, by the average of salaries

* `numpy.nan` : this respresents all the missing values in a CSV  
https://numpy.org/doc/stable/reference/constants.html#numpy.nan

* `strategy='mean'` : Replace the missing_values with the average

* ``fit(NUMERICAL_VALUES)`` It looks for the missing value and also calculate the replacement.
___

## 1.6. Categorical Data
What are categorical variables and how to encode categorical data, which is illustrated in Python by `LabelEncoder` and `OneHotEncoder` class from `sklearn.preprocessing` library, and in R the factor function to transform categorical data into numerical variables.

### 1.6.0. Label Encoder vs. One Hot Encoder (More)
So in the first column (Country) we have some countries which can't be understood by our ML Model. So we should make them number, we could have do that with Label Encoder which gives for example to French, Germany, Spain these numbers: 1,2,3  

Now the Problem is that our MODEL is going to misunderstand this, because it's going to compare these values! But we know that the Countries can not be compared like that! So what can help us here is ``One Hot Encoder``! This makes the string data numeric without allowing them to be compared!

(Mori: One-Hot-Encoder make a Unity Matrix (`n * n`) out of the categorical values. Where `n` is going to be the number of values)

IN OUR CASE the OneHotEncoder splits the Country-Column into THREE columns, because we have also 3 countries (If we had 5 Countries, the Country-Col would have been splitted into 5 columns).

THE ONE-HOT-ENCODER CREATES ``BINARY VECTORS`` FOR EACH COUNTRY.

### 1.6.1. Encoding Independent Variable (Features - X)
#### ``sklearn.compose.ColumnTransformer``
* Instructor about Line-32 to Line-34 : We have to enter two arguments:
    1. ``transfomers`` : with that we specify what kind of transformation we are going to do and which indexes of column we want to transform
    2. ``remainder='passthrough''`` : which specifies we actually want to keep the columns which won't get this transformation, meaning "Age" and "Salary" untouched!
    
    3. More details: ``tansformer=[(KIND_OF_TRANSFORMATION, TYPE_OF_ENCODING, COLUMNS_TO_BE_APPLIED)]``
    
    4. `fit_transform()` : It does the both FITTING and TRANSFORMING at once! (This was not possible by ``imputer``, so there we have first used ``.fit()`` and then ``.transform()`` in LINE-24 and LINE-28)
    
    5. LINE-38 : We use ``numpy.array()`` because ``ColumnTransformer()`` does not do that for us. And it should be a Numpy-Array, otherwise our Model can not train with it!
    
    6. _You now know to apply the One-Hot-Encoding when you have several categories in the matrix of features, but also you can do a simple Label-Encoding when you have two classes which you can directly encode to 0 and 1, in other word BINARY OUTCOME._

* https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html

* This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated TO FORM A SINGLE FEATURE SPACE. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.

* Parameter "transformers" : list of tuples  
List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.

* Parameter "remainder" , also called ESTIMATOR.

#### ``sklearn.preprocessing.OneHotEncoder``
* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder

* Encode categorical features as a one-hot numeric array.

* The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter)
___

### 1.6.2. Encoding Dependent Variable (y)
#### ``sklearn.preprocessing.LabelEncoder``
* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.htm  

* This transformer should be used to encode target values, i.e. ``y`` and not the input ``X``.

* It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.

(Mori:
    * Why is it ok to use LabelEncoder for a Binary Column and not OneHotEncoder?
    
    * Are binary values (True & False) comparable?! In my mind they are two independent states

    * If we had ONLY two values in our categorical columen (for example: category = country , values: France, Germany) would it have been ok to use LabelEncoder instead of OneHotEncoder? Would it have lead to a false training for our Model?
    
    * Is this not ok if we use OneHotEncoder on Dependent Columns?
)
ANSWER: https://en.wikipedia.org/wiki/Binary_data#In_statistics
___

## 1.7. Splitting Dataset into the Training-Set and Test-Set

__Why__ we should split data into training-set and test-set BEFORE feature-scaling?  
(Feature Scaling simply consists of: _Scaling all your variables (features) to make sure they all take values in the same scale. This should be done to prevent one feature domainates the others, which therefore would be neglected by the ML Model._)  

__Answer:__ The reason is simple. Because the Test-Set is supposed to be a __brandnew-set__, on which you are going to evaluate your ML model.  
Feature Scaling is a technique to get the __average__ and __deviation__ of the feature to perform scaling. So if we do this before splitting the Test-Set it causes an __Information Leakage__. (Mori: As if our Machine Learning is cheating!!!)

### 1.7.1 Splitting the Dataset into the Training and Test Set (Python)
In Machine Learning we split our data to a Training-Set and a Test-Set.  
You know that this is about the machine which is going to learn something to make predictions.

Imagine your machine learn too much on a dataset. Then we are not sure if its performance is great on a new set with a slightly different corrolations.

So we should always keep a part of data for the Test!

The performance of the Machine should not be that much different on the Test-Set comparing to Training-Set, so then we can conclude that this Model can understand Correlations (And he did not learn it by heart!) and so it can adapt the new sets of data in new situations.

BETWEEN 20% TO 30% OF DATA IS A GOOD CHOICE FOR THE TEST-PART!

### 1.7.2. How Machine Learns Now?
Now the Machine Learning Model is going to find a CORRELATION between the X_train and y_train and with this Correlation it can predict a new_y for a new_X! or we can test it the quality of its Prediction with the X_test and y_test

___
### 1.7.3. WARNING - Update (Part 1-8)
WARNING - Update
Dear students,

in the following tutorial, the first line of code we will type will be:

`from sklearn.cross_validation import train_test_split `

However the "cross_validation" name is now deprecated and was replaced by "model_selection" inside the new anaconda versions.

Therefore you might get a warning or even an error if you run this line of code above.

To avoid this, you just need to replace:

``from sklearn.cross_validation import train_test_split`` 

by
``from sklearn.model_selection import train_test_split`` 
___

## 1.8. Feature Scaling
### WHAT - Definition
__Feature Scaling is a technique that will put all your features in the SAME RANGE.__    

If we look at our ``Data.csv`` we can clearly see that the values of the Age-feature are NOT in the same RANGE as the values of the Salary-feature the Age-range is from like 0 to 100 and the Salary-Range is from 0 to 100 thousand.

Now we want to put the Age-Range and the Salary-Range in a same range using FEATURE SCALING technique.


### WHY - should we apply Feature Scaling?
For some of the ML Models (not all of them) if your different Features have a huge difference in range of their values, this can cause a __BIAS IN THE CORRELATIONS COMPUTATIONS__.  
In another word, the features that have higher values compared to the other ones will DOMINATE the other features so that these other features may NOT BE CONSIDERED in the Correlation Computation.

So depends on our Model sometimes we need to apply the Feature Scaling and sometimes it is not necessary, because the Model automatically can detect this issue and they fix this with ADAPTING the COEFFICIENTS (For example you see that with the Linear Regression. Linear Regression has some coefficients for each of the Features, so the Features with super HIGH VALUES will get a very LOW COEFFICIENTS. But for other regressions like Logistic Regressions or also the ML Model in R, we should apply the Feature Scaling)

### HOW - Feature Scaling Methods
* Standardisation : taking each value of the feature and subtract it by the Mean and then divided by the Standard Deviation. This puts all the values in range of usually between -3 and +3.

* Normalisation : In this we subtract the values of Feature by the Minimum value of the Feature and then divided by the Range (Max - Min). This will put all the values in the Feature between 0 and 1.

![feature scaling](./images/feature_scaling.png)

#### Standardisation vs. Normalisation
Which one to choose? Normalisation is recommended if you have a normal distribution in most of your features.  

Standardization actually works well all the time. So we are going to use __Standardization__.

VERY IMPORTANT: For Standardization we need "mean" . We are going to get the Mean of X_train and use it for standardization of both Training-set and also Test-Set (WARNING: We should not take the mean of X_test, because it would - as in the previous lecture said - lead to INFORMATION-LEAKAGE)


``StandardScaler()`` keeps the X as numpy.array so we don't need to apply ``numpy.array()`` in LINE-48. 


ONE IMPORTANT QUESTION: Do we need to apply our Feature-Scaling (here: Standardization) to the Matrix of Features (OneHotEncoder)? The ANSWER is NO! Because the goal of Feature-Scaling is to have value of all features in a same range (somewhere between -3 and +3) for our categorial data which are now vectors of size 1 we don't need to apply the Feature-Scaling for them. Standardization would even make them worse (dummy variable)!

Categorical Data =====(OneHotEncoding)=====> Dummy Variables (Mori: bzw. Einheitsvektoren in N*N Raum!)

### Difference between fit() and transform()
`fit()` __calculates__ the standard-deviation and the mean of each column  

`transform()` maps each cell using the scaling method to its new value (using equation in the above picture)

so X_train looks like this:

```python
[
    [0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]
    [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]
    [1.0 0.0 0.0 0.566708506533324 0.633562432710455]
    [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]
    [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]
    [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]
    [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]
    [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]
```

and X_test looks like this:

```python
[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]
 [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]
```

We can see that Feature-scaling worked and they are all in the same range (age and salary)
___

# 2. Linear Regression
Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.

In this part, you will understand and learn how to implement the following Machine Learning Regression models:

* Simple Linear Regression
* Multiple Linear Regression
* Polynomial Regression
* Support Vector for Regression (SVR)
* Decision Tree Classification
* Random Forest Classification

## 2.1. Simple Linear Regression Intuition - Step 1
The linear equation and the name of its components:
![linear_equation](./images/linear_regression_01.png)

So we are going to look at an Example for Linear Regression: In the following we want to know homw the ``salary`` of employees in a company depends on their ``experience``.

![linear_regression_example](./images/linear_regression_02.png)

### 2.1.1 Explaining The Above Diagram 
So now let's look at the simple in your regression because it's the easiest one to discuss. It's very pretty straightforward you can visualize it quite well.

So here we got the y and x axis. Let's look at that specific example where we have EXPERIENCE and SALARY. So experience is going to be our horizontal axis. Salary is all vertical axis and we want to understand how people's salary depends on their experience.

WELL WHAT WE DO IN REGRESSION IS WE DON'T JUST COME UP WITH A THEORY WE LOOK AT THE EVIDENCE WE'LL LOOK AT THE LIVE HARD FACTS SO HERE ARE SOME OBSERVATIONS WE'VE HAD.

So in a certain company this is how salaries are distributed among people who have different levels of experience and what a regression does.

So that's a formula for aggression. In our case it'll change to salary equals be zero plus ``b_1`` Times EXPERIENCE.

AND WHAT THAT ESSENTIALLY IT MEANS IS JUST PUTTING A LINE THROUGH YOUR CHART THAT BEST FITS THIS DATA and we'll talk about best fitting in the next tutorial when we're talking about ordinary squares.

But for now this is the chart. This is the line that best fits as Darren even looks like it right.
___

### 2.1.2 Coefficient ``b_0``
For now let's focus on the coefficients and the caffeine and the constant.

So what does the constant mean here. Well the that actually means the point where the line crosses the vertical axis and let's say it's $30000.

What does that mean. Well it means that when when experience is zero. So when as you say on the horizontal axis when experience is at zero in the formula on the right you can see that the second part ``b_1`` Times experience becomes zero so salary equals zero.

That means that salary will equal to $30000 when a person has no experience so soon somebody is know fresh from University and joins this company. Most likely they will have a salary about $30000.
___

### 2.1.3. Coefficient ``b_1``
Now what is ``b_1``, ``b_1`` IS THE SLOPE OF THE LINE. 

AND SO THE STEEPER THE LINE THE MORE YOU GET MORE MONEY YOU GET PER EXTRA YEAR OF EXPERIENCE.

Let's look at this. In this particular example let's say somebody went from I don't know maybe four to five years of ``experience``. So then to understand how his salary increase you have to project this onto the line and then project that onto the salary access and you can see that here for one of your experience the person will get AN EXTRA TEN THOUSAND DOLLARS ON TOP OF HIS SALARY.

So if the coefficient ``b_1`` is less, then the slope will be less and that means the salary increase will be less per every year of experience. If the slope is greater then that means the experience will yield more increase in salary and that's pretty much it.
___

That's how a simple your regression works. So the core goal here is that we're not just drawing a line theoretically that we can we came up with

SOME HOW WE'RE ACTUALLY USING OBSERVATIONS THAT WE HAVE TO FIND THE BEST FITTING LINE AND WHAT BEST FITTING LINE IS WE'LL TALK ABOUT THAT IN THE NEXT TUTORIAL.
___

## 2.2. Simple Linear Regression Intuition - Step 2

![linear_regression_HOW_works](./images/linear_regression_03.png)

### 2.2.1. How Find Best Fitting Line
HOW THE LINEAR REGRESSION BEING A TREND LINE THAT BEST FITS YOUR DATA.

Today we'll find out how to find the best fitting light or in fact how the simple linear regression finds that line for you.

So here's our simple your aggression. The same chart salary versus experience. We've got these red dots which represent the actual observations that we have in our data and we've got the TREND LINE WHICH REPRESENTS THE BEST FITTING LINE OR THE SIMPLE LINEAR REGRESSION MODEL.

So now let's draw some vertical lines from the actual observations to the model. And let's look at one of the specific examples to understand what we're talking about here. 

So here you can see that the Red Cross is where that person is sitting at in terms of salary so let's say this person with 10 years of experience is earning $100000.

__-Interpretation-__  
WELL THE MODEL LINE (THE BLACK LINE), IT ACTUALLY TELLS US WHERE THAT PERSON SHOULD BE SITTING ACCORDING TO THE MODEL IN TERMS OF SALARY AND ACCORDING TO MODELS SHOULD BE A BIT LOWER. It should be somewhere without green crosses which is about maybe let's say thousand.

### 2.2.2. Green And Red Pluses in Diagram:
So now the Red Cross is called ``y_i``. And that is the ACTUAL DURATION.

The Green Cross is called ``y_î`` (Y_i-hat)  is THE MODEL THE OBSERVATIONAL. or THE MODELED VALUE.

So basically with those that level of experience where would he be. Where does the model predict that he would be earning.

And so the green line therefore is the difference between what he's actually earning and what he should be earning.

So it should be what he's modeled to be earning. So therefore the green line will be the same regardless of what dependent variable you have whether it's salary or with it's grade school whatever. So it's the difference between the observed and the modeled for that level of independent variable.

### 2.2.3. How Linear Regression Works!
Now to get this best fitting line what is done is you take the sum you take each one of those green lines are those distances (``y_i - y-î``) you square them and then you take some of those squares.

Once you have the sum of the squares for you got to find the MINIMUM of this ``SUM``!

So basically what a simple linear regression does is it draws lots and lots and lots of these lines. These trend lines all this is like a simplistic way of imagining the linear regression draws all these all possible trend trend lines (_Mori: Trend Lines are those vertical green lines!_) and counts the sum of those squares every single time.

And it store these SUMs somewhere in a temporary you know file or something like that and then 

IT FINDS THE MINIMUM ONE SO IT LOOKS FOR THE MINIMUM SUM OF SQUARES AND FINDS A LINE WHICH HAS THE SMALLEST SUM OF SQUARES POSSIBLE.

and that line will be the best fitting line and that is called the ordinary least squares method.

So that's how the simple linear regression works and look for you on the next tutorial.

___

## 2.3. Simple Linear Regression in Python - Step 1
In the example `Salary_Data.csv` we have 30 dataset and the goal is to create a linear regression based on Years-of-Experience (Indep. Variable X) and Salary (Depend. Variable Y). And using that our model should predict other salaries based on other years of experiences.

In this lecture we implement:
1. Import CSV using pandas
2. Set the X and y
3. Split dataset into training-set and test-set
___

## 2.4. Simple Linear Regression in Python - Step 2
Now in this lecture we want to TRAIN our model!

__Regression vs. Classification__  
Regression is when our model predict a continuous real value (e.g. Salary). Classification is when the model predict the category/class (We attend it in the comming lectures).

`LinearRegression.fit()` : This function train our model (`regressor` in code)
___

## 2.5. Simple Linear Regression in Python - Step 3
``LinearRegression.predic()`
___

## 2.6. Simple Linear Regression in Python - Step 4
__Visualization__  
We want make plot out of our dataset and the prediction.

`pyplot.scatter(X_train, y_train, color='red')` : This will plot the red points corresponding to the real salleries in a 2D plot.

Now we want to plot the Regression Line. For that we use `pyplot.plot()`

`pyplot.show()` : It displays the graphic.

The result of these two plots:

For training set the line should looks already good, because the model has trained with these dataset

![train set](./images/plot_2-6)

But important is the Regression Line on the Test-Set! and it looks acceptable:

![test set](./images/plot_2-6-2)

Please notice that these precise lines won't happen for all the datasets! Here our dataset had intentionally some linear correlations between salary and expreience!!
___

## 2.7. Simple Linear Regression in Python - Additional Lecture
Hello friends!

When discussing Linear Regression, two questions arise frequently from data science students, and now we’re showing you how to solve them in a free BONUS exercise.

Question 1: How do I use my simple linear regression model to make a single prediction, for example, to predict the salary of an employee with 12 years of experience?

Question 2: How do I get the final regression equation y = b0 + b1 x with the final values of the coefficients b0 and b1?

To help you tackle these common challenges, I’ve created a BONUS that addresses the concepts and steps you need to solve them—code included!

Access the detailed answers to two of our most frequently asked questions in this bonus Colab file [here](https://www.superdatascience.com/pages/ml-regression-bonus-1):

https://www.superdatascience.com/pages/ml-regression-bonus-1

Enjoy this BONUS!
___

# 3. Multiple Linear Regression

## 3.1. Dataset + Business Problem Description
This is like a very realistic life-like challenge.

* We have 50 companies
* The first 3 features are their costs. (i.e. R&D Spend, Administration, Marketing Spend)
* One categorical feaature, i.e. "state"
* and the dependent feature (y), which our Model should at the end predict: __Profit__

Aufgabe: Ein Investition-Unternehmen (Venture-Kapitalist) hat Sie als Data-Scienstist eingestellt um herauszufinden, Investieren in welche
von diesen 50 Firmen sinnvoller waere (auf der Basis von "Profit")  

Venture-Kapitalist möchte auch feststellen:
* Wo eine Firma besser leistet? (zB in Kalifornien oder New York ?)
* Wie ist die direkte Beziehung zwischn "Marketing Spend" und "Profit" (wenn alle andere Feaatures gleich sind) ?
* Die Ausgabe in "R&D Spend" und "Marketing Spend" , welche ergibt in einen besseren Profit?
___

## 3.2. Multiple Linear Regression Intuition - Step 1
Watch Video. Topic: Multi-Linear Equation

* Multilinear-Regression: it has one Dependent Variable (DV - y) and multiple Independent Variables (IVs - x_1 , x_2, x_3 , ... , x_n)
___

## 3.3. Multiple Linear Regression Intuition - Step 2
**Assumptions of Linear Regressions:**

1. Linearity
2. Homoscedasticity
3. Multivariate normality
4. Independence of errors
5. Lack of multicollinearity
___

## 3.4. Multiple Linear Regression Intuition - Step 3
**Dummy Variables:**

* "state" is a categorical variables
* To let a categorical variable participate in a regression, you should make __dummy variables__ out of it.
___

## 3.5. Multiple Linear Regression Intuition - Step 4
You should always have one dummy variable less than number of your categorical values of the feature. For example if you have 5 States , you should have 4 dummy variables.
___

## 3.6. Understanding the P-Value
**Statistical Significance**

There's always a question you should ask yourself: _Are my results statistically significant or not?_  

Let's take the example: "Coin Toss" (Sekke bala andakhtan)  
We want to imagine two possible hypothesis:  
1. H_0 : This is a fair coin
2. H_1 : This is not a fair coin

We assume that H_0 is true. (So it's a fair coin)  
In this hypothesis the occuring of head/tail is 50%  

imagine the following occurances:

* First-Time: it's tail (probability: 0.5)
* Second-Time: it's again tail! (probability of these two occurances: 0.25)
* Third-Time: it's again tail!! (p: 0.12)
* Fourth-Time: it's again tail!!!! (p: 0.06)
* Fifth-Time: it's again tail!!!!! (p: 0.03)
* 6th-Time: it's tail again!!! (p: 0.01)

and these values are **P-Value**

![p value](./images/p-value.png)

In the above occurances you may have been got suspicious that we are really in universe of H_0 (you start thinking that it's maybe H_1). So you could have say "Any probabiliy under 5% is not random (is not H_0 anymore)" so __5%__ is your STATISTICAL SIGNIFICANCE here. So you reject the happening of 5 or 6 time Tail behind each other and you actually reject that this is the Universe with H_0 hypothesis and you claim in such a situation you are in H_1 hypothesis.

But in the H_1 universe the probablities would have been different. For example occuring of 6 times Tail behind each other could be 35% !
___

## 3.7. Multiple Linear Regression Intuition - Step 5
__Building A Model (Step by step)__

5 methods of building models:  
* All-in
* Backward Elimination 
* Forward Selection 
* Bidirectional Elimination 
* Score Comparison

**Note:** sometimes the steps 2,3,4 are called: __Stepwise Regression__.  
Sometimes only step 4 is called Stepwise Regression.

### 3.7.1. "All-in" cases
This is where you throw all the variables to your model. For example in case of:  

* Prior knowledge: if you know that these exact variables are your true predictors. You might know it from domain knowledge or etc...

* You have to! Maybe there are some frameworks in your company that compelse you to use all those variables. In another word, when it's not your decision to choose the variables.

* Preparing for the __Backward Elimination__

### 3.7.2. Backward Elimination
Steps of backward elimination:  
1. Select a __significance level__ to stay in the model (e.g. SL = 0.05)
2. Fit the full model with all possible predictors (put all variables in your model)
3. Consider the predictor with the **highest** P-value. If `P > SL`, go to next step, otherwise END (Your model is ready!).
   (Mori: shouldn't it be `P < SL`?!)
4. Remove the predictor
5. Fit model without this variable*
6. Go back to Step 3.

### 3.7.3. Forward Selection
1. Select a significance level to enter the model (e.g. SL = 0.05)
2. Fit all simple regression models `y ~ x_n`. Select the one with the lowest P-value
3. Keep this variable and fit all possible models with one extra predictor added to the one(s) you already have.
4. Consider the predictor with the lowest P-value. If `P < SL`, go to STEP 3, otherwise END. (Kepp the previous Model, because the currenct Model has an insignificant variable with P > SL)

### 3.7.4. Bidirectional Elimination
1. Select a significance level to **enter** and to **stay** in the model. e.g.: SLENTER = 0.05 , SLSTAY = 0.05
2. Perform the next step of the Forward Selection (new variables must have: **P < SLENTER** to enter)
3. Perform ALL steps of Backward Elimination (old variables must have **P < SLSTAY** to stay)
4. Go back to Step2
5. In some point of the process: **No new variables can enter and no old variables can exit** . So it's END. Your model is ready!
___

Another approach: __All Possible Models__  
This is the most-resource-consuming approach.  
Steps:

1. Select a criterion of goodness of fit (e.g. Akaike criterion)
2. Construct all possible regression models: `2^n - 1` total combinations
3. Select the one with the best crietrion
4. Your Model is Ready

It sounds easy but imagine you have 10 independent variables, you would then have 1023 MODELS!!!!!

In this course we are going to use __Backward Elimination__, because it's the fastest one out of these methods.
___

## 3.8. Make sure you have your Machine Learning A-Z folder ready
empty
___

## 3.9. Multiple Linear Regression in Python - Step 1
As we know the first phase to create and train the model is __data pre-processing__

In feature __State__ we have 3 categories (states) which needs one-hot-encoding (dummy variables).
___

## 3.10. Multiple Linear Regression in Python - Step 2
__Data Preprocessing__

Important Note: In Data-Preprocessing here we do NOT NEED the __feature-scaling__. Because the coefficients (b1, b2, b3, ..., bn) of the independent variables (x1, x2, x3, ..., xn) will compensate the difference range of values in the data. 
___

## 3.11. Multiple Linear Regression in Python - Step 3
Two important notes before creating and training the Model using multilinear regression:

* We won't be affected by Dummy-Variable-Trap , because the `LinearRegression` class will take care of that.

* We also don't need to use a method like __Backward Elimination__. Because again the `LinearRegression` class will calculate the best features (features with highest P-values, which are statistically most significant) by itself!

GOAL: In the next leacture We will compare the vector of the 10 real profits of the test set (20% of 50 samples) and then compare it to the 10 predicted profit of these 10 test samples
___

## 3.12. Multiple Linear Regression in Python - Step 4
`.reshape(LENGTH_OF_VECTOR, 1)` : `1` means only one column, so in this way instead of printing horizentally, we print our vector vertically!

We want to concatanate ech row of y_test and y_pred (predict) to compare them.

```
[
 [103015.2  103282.38]
 [132582.28 144259.4 ]
 [132447.74 146121.95]
 [ 71976.1   77798.83]
 [178537.48 191050.39]
 [116161.24 105008.31]
 [ 67851.69  81229.06]
 [ 98791.73  97483.56]
 [113969.44 110352.25]
 [167921.07 166187.94]
]
```

we can see that the first and last results are very good predictions but the others are not that exact, but still ok.

So our Multi-Linear-Reg Model worked rather well!
___

## 3.13. Multiple Linear Regression in Python - Backward Elimination
Hi my friends,  

As I explained in the previous tutorial, Backward Elimination is irrelevant in Python, because the Scikit-Learn library automatically takes care of selecting the statistically significant features when training the model to make accurate predictions.  

However, if you do really want to learn how to manually implement Backward Elimination in Python and identify the most statistically significant features, please find in this link below some old videos I made on how to implement Backward Elimination in Python:  

https://www.dropbox.com/sh/pknk0g9yu4z06u7/AADSTzieYEMfs1HHxKHt9j1ba?dl=0  

These are old videos made on Spyder but the dataset and the code are the same as in the previous video lectures of this section on Multiple Linear Regression, except that I had manually removed the first column to avoid the Dummy Variable Trap with this line of code:  

```python
# Avoiding the Dummy Variable Trap
X = X[:, 1:]
```

Just keep this for this Backward Elimination implementation, but keep in mind that in general you don't have to remove manually a dummy variable column because Scikit-Learn takes care of it.

And also, please find the whole code implementing this Backward Elimination technique:  

```python
# Multiple Linear Regression
 
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
 
# Importing the dataset
dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
print(X)
 
# Encoding categorical data
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
print(X)
 
# Avoiding the Dummy Variable Trap
X = X[:, 1:]
 
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
 
# Training the Multiple Linear Regression model on the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)
 
# Predicting the Test set results
y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
 
# Building the optimal model using Backward Elimination
import statsmodels.api as sm
X = np.append(arr = np.ones((50, 1)).astype(int), values = X, axis = 1)
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()X_opt = X[:, [0, 1, 3, 4, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()X_opt = X[:, [0, 3, 4, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()X_opt = X[:, [0, 3, 5]]
X_opt = X_opt.astype(np.float64)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()X_opt = X[:, [0, 3]]
X_opt = X_opt.astype(np.float64)regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()
```

Once again, this is totally optional.

Hope that can help anyway, and until then, enjoy Machine Learning!
___

## 3.14. Multiple Linear Regression in Python - EXTRA CONTENT
Hello friends!

When discussing Multiple Linear Regression, many questions arise frequently from data science students, and now we’re showing you how to solve them in a free BONUS exercise.

**Question 1:** How do I use my multiple linear regression model to make a single prediction, for example, the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = California?

**Question 2:** How do I get the final regression equation y = b0 + b1 x1 + b2 x2 + ... with the final values of the coefficients?

To help you tackle these common challenges, I’ve created a BONUS that addresses the concepts and steps you need to solve them—code included!

Access the detailed answers to two of our most frequently asked questions in this bonus Colab here (Mori: I added it to `ch3_app.py`):

https://www.superdatascience.com/pages/ml-regression-bonus-2

Enjoy this BONUS!

__IMPORTANT from LINE 80:__ _the dummy variables are always created 
in the first columns._
___

# 4. Polynomial Regression

## 4.1. Polynomial Regression - Intuition
The formula of a polynomial regression looks like this:

`y = b_0 + b_1 * x_1 + b_2 * (x_1 ^ 2) + b_3 * (x_1 ^ 3) + ... + b_n * (x_1 ^ n)` (this is called Polynomial Linear Regression because there's no other indep. var like x_2)
___

## 4.2. Polynomial Regression in Python - Step 1
Data set (`Position_Salaries.csv`) : Imagine we are in a HR department and we want to hire someone. And we find out that a person would be a great fit for the position. And imagine the person is advanced and therefore asks for 160K dollor per year!  
As the HR we will ask person why he expects such a high salary? Person replies: "Because that is what i earned in my previous company."  

Is that a truth? or is that a bluff? Well, that's exactly what we are going to figure out with the help of __Polynomial Regression Model__.  
We are going to build the poly. reg. Model __to figure out the previous salaries of this candidate__.

In order to do such a prediction we have collected the salaries of the previous company of this candidate for different positions. And then we want to find out the __Salary for the `Level` 6.5__ (because the Person has been Region Manager in his company for around 2 years so his `Level` is more than 6.5)


__IMPORTANT:__ Because here we don't have a lot of data, we WILL NOT split the data-set into training-set and test-set. Because we want to use the model to find out the __Salary for the Level 6.5__


__IMPORTANT:__ The columns `Position` and `Level` are redundant. Because the `Level` is representing the `Postion` so we don't consider `Position` in the `X`.
___

## 4.3. Polynomial Regression in Python - Step 2
For Polynomial Linear Regression, we consider each of x_1 , x_1 ^ 2 , x_1 ^ 3 , ... x_1 ^ n as an independent variable in the matrix of features. For that `PolynomialFeatures` class helps us. and we will cal it `X_poly`

`PolynomialFeatures(degree=2)` this means: `y = b_0 + b_1 * x_1 + b_2 * x_1 ^ 2`

For X_poly we can now use a new instance of Linear-Regression Model to train! That's why we call it "Polynomial Linear Regression"! Because you can map such a polynomial equation into a [multiple] Linear Regression.  
___

## 4.4. Polynomial Regression in Python - Step 3
Now we want to visualize the results.

![4-4 lin reg](/images/4-4-lin-reg.png)

So we see that the prediction is very inaccurate above.

![pol reg](/images/4-4-pol-reg.png)

Now if we increase the degree from 2 to 4 we see even the prediction gets better!
___

## 4.5. Polynomial Regression in Python - Step 4
We are going to predict the Salary of Level=6.5 using the both linear and polynomial model
___

# 5. Support Vector Regression (SVR)

## 5.1. SVR Intuition
* SVR = Support Vector Regression
* Invented by Vladimir Vapnik and his collegues in 1990s.

![svr explanation](images/svr-01.png)

In the above picture we have the simple linear regression and how it is calculated on left.  
The SVR (Support Vector Regression) is also very similar to that. But there we have an area above and under the linear regression line, in which we ignore the data we have to calculate our regression line, which is called __Insensitive Tube__.

As a supervised-learning approach, SVR trains using a symmetrical loss function, which euqally __penalizes__ high and low misestimates. Using Vapnik's insensitive approach, a flexible tube of minimal radiu i formed symmetrically around the estimated function, such that the abolute value of errors less than a certain threshold are ignored both above and below the estimate. In this manner, points outside the tube are penalized, but those within the tube, either above or below the function, receive no penalty. __One of the main advantages of SVR is that its computational complexity does not depend on the dimensionality of the input space. Additionally, it has excellent generalization capability, with high prediction accuracy.__  

__MORI:__ _So I think that's not true what the tutor in this lecture said ("inside the tube the points are going to have no effects"). They are going to have their part in calculating the Sum of Minmum y distances but outside of this tube the other points are going to suffer a penalty and that's the reason that at the end of this chapter we see in the visualization that the last point (Salary: 1Million for PositionLevel 10 does not effect the prediction curve that much, as it did in the last chapter where we OVERFITTED our polynomial regression using Power 4 or 5 of X)_ 

Read more here:

![img.png](images/svr-02.png)
___

## 5.2. Heads-up non-linear SVR
It will be covered later in this course:

* Section on SVM:
  * SVM Intuition
* Section on Kernel SVM:
  * Kernel SVM Intuition
  * Mapping to a higher dimension
  * The Kernel Trick
  * Types of Kernel Functions
  * Non-linear Kernel SVR

Either you can read this parts and continue or you can continue the course but keep in mind that it's going to use non-linear SVR.
___

## 5.3. SVR in Python - Step 1
In this section our model and data are more advance.  
In this section we are going to use __feature scaling__ a lot!

Goal: In this section we want that Model learns and predicts the correlation between the `Position-Level` and the `Salary`.

(Like the last chapter) We want hire a new person, who expects 160K $ per year. And he claims that it's the salary he got in his last company for the post __Region Manager__ (Level=6). Somehow we could have find data about his last company: `Position_Salaries.csv`. However, as he has some years of experience his level is not 6 but something like `6.5`

In SVR we MUST apply Feature-Scaling because here we DON'T have the coefficients to COMPENSATE the values of the features when they are not in the same scales.

We will use the whole dataset for training to maximise the prediction strength.
___

## 5.4. SVR in Python - Step 2
As the scale of values of salary (45000 to 1000000) is very higher than the values of position levels (1 to 10), the position level could be neglected compared to salary.  
So we are going to apply the feature-scaling to both our independent variable x (position-level) and also to our dependent variable y (salary).

__IMPORTANT:__ We need two instances of `StandardScaler()` class, one for x and one for y. Because when you fit an instance of StandardScaler() on your data, this is going to **compute the mean and standard deviation** of that same variable. Of course the `PositionLevel` and the `Salary` have different values for mean and standard deviation, so we NEED TWO instances of `StandardScaler()`.
___

## 5.5. SVR in Python - Step 3
SVR use something called __Kernel Function__, which can either learn some linear relationships and that's the linear kernel or non-liner relationships, which are the non-linear kernels such as the followings (https://data-flair.training/blogs/svm-kernel-functions/):

* __Polynomial Kernel__ - adapted for non-linear dataset
* __Gaussian Kernel__ - which is a classic gaussian function
* __Gaussian Radial Basis Function (RBF)__ - the most widely used one (and this is the one we are going to use now)
* __Laplace RBF Kernel__
* __Hyperbolic Tangent Kernel__ - is a popular kernel
* __Sigmoid Kernel__
* __Bessel Function of First Kind Kernel__
* __Anova Radial Basis Kernel__
* __Linear spline kernel in 1D__

The __Gaussian RBF__ is also recommendation of the course's tutor when the SVR is used for regression `SVR(kernel='rbf')`.
___

## 5.6. SVR in Python - Step 4
If you run into ValueError when Predicting A New Result in the colab notebook for SVR please use the following snippet to reshape :

`sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]])).reshape(-1,1))`

Now as we scaled our `x_target` so the output `y_predict` is also scaled. To reach the right prediction we should reverse the feature-scaling on Y: `sc_y.inverse_transform()`
___

## 5.7. SVR in Python - Step 5
For the visualization we should INVERSE the feature-scaling we applied on X and y to see the correct values on the diagram.

![img.png](images/svr-03.png)
___

___

# 6. Decision Tree Regression

## 6.1. Decision Tree Regression Intuition
You may have heard "CART" which stands for Classification and Regression Tree. We are going to talk about both, but first in this chapter we talk about the Regression Tree.

![img.png](images/decision-tree01.png)

Imagine we have the above scatter plot (3D diagramm, the Y is sticking out of the screen and for simplicity is not presented here.). The `X_1` and `X_2` are two independent variables.  

Using __decision tree__, a scatter plot will be split up into segments. The algorithm would work for example as the following:

_It's going to split the scatter plot by `X_1 < 20` and then it's going to split the `X_1 > 20` into two segments `X_2 > 170` and `X_2 < 170`. and then so on Split3 and Split4 (as shown in the following picture)._

![img.png](images/decision-tree02.png)

The splitting will goes on, __until adding a new segment and grouping the dataset would not give us a new information__. So then it will stop splitting up the dataset.

These segments (for example in the above pciture we have 5 segments) are called __leaves__. and the final leaves are called __terminal leaves__.  

And then this is how the __prediction__ works according to the decision-tree method: __The average of the Y (dependent variable) in each __terminal leaf__ will be calculated. So when we add a new "dot" (dataset) to this scatter plot depending in which terminal-leaf it's going to be, the __prediction of its Y is going to be the Y-average in that terminal-leaf!__

For example imagine that we have calculated the average of each terminal-leaf in the above picture and it gives us:

![img.png](images/decision-tree03.png)

so the Decision Tree of it looks like this:

![img.png](images/decision-tree04.png)
___

## 6.2. Decision Tree Regression in Python - Step 1
Again we are going to have the same dataset `Position_Salaries.csv` like in the last two chapters, so for more explanation of the problem and the challenge. Read the first section of the each two last chapters.

* __NOTICE 1:__ The Decision Tree Regression is not well adapted to simple datasets like here with only one feature (independent variable)! We will see explain the problem in the visualization lecture. BUT you should not worry, because the implementation of Decision-Tree-Regression Model which we are about to build will still work on any dataset with any number of independent variables (features)

* __NOTICE 2:__ You don't need to apply any __feature-scaling__ on the decision-tree-regression (and random-forest in the next chapter) model!

* __NOTICE 3:__ You can but you do not need to split your data into training and test set. Because in the decision-tree and random-forest approach the __prediction is resulting from successive segmentation of the data through different nodes of your tree, so there will not be any equations like the previous models and that's also why no FEATURE SCALING is needed to split different values of features.__
___

## 6.3. Decision Tree Regression in Python - Step 2
Set a value for `random_state` parameter is only for the reason to receive the same result as in the course videos.
___

## 6.4. Decision Tree Regression in Python - Step 3
Now it returns 150K dollar for PositionLevel 6.5 , which is NOT a good prediction. BUT if you remember we said at the beginning of this chapter that a scatter plot with only one feature is not a good use case for using decision-tree regression model.
___

## 6.5. Decision Tree Regression in Python - Step 4
You can see the result here and why the Decision-Tree Regression Model is not suitable for such a dataset with only one feature:

![img.png](images/decision-tree5.png)
___

___

# 7. Random Forest Regression

## 7.1. Intuition
* Random Forest is a version of __Ensemble Learning__
* Ensemble Learning is when you take **multiple algorithms** or the **same algorithm multiple times** together, to make something much powerful than the original algorithm.

### Steps:
1. Pick a random `K` data points from the Training set.
2. Build a **Decision Tree** associated to these `K` data points.
3. Choose the number N-tree of trees you want to build and repeat STEP 1 and STEP 2
4. For a new data point, make each one of your N-tree trees predict the value of `Y` to for the data point in question, and assign the new data point the average across all of the predicted `Y` values

In this way you are not predicting based on one __tree__, but predicting based on a __forest of trees__. This __improves the accuracy__ of prediction.  

Generally __Ensemble algorithms__ are more __stable__, because the changes in a dataset could impact its tree but it could not affect the __forest__.
___

## 7.2. Random Forest Regression in Python
In `RandomForestRegressor()` the important parameter is `n_estimators` which sets the __number of trees__ the datasets are goint to split to them.    

The goal is to find the best value for the `n_estimators` but first we can start with `10`.

WARNING: Just like the Decision-Tree, the Random-Forst regression is best for the high dimensional datasets (multi features dataset), so the result of it for our currenct dataset `Position_Salaries.csv` which has only one X (one feature) is not going to be that pretty.

![Random Forest](images/random-forest.png)
___

___

# 8. Evaluating Regression Models Performance

## 8.1. R-Squared Intuition
![r squared](images/r-squared_01.png)

Until now we have learned how the regression lines or curves are calculated: _Minimizing the Sum of squared dataset-Ys minus the y on the regression line._:

`SUM (y_i - y^_i)^2 --> Min`

The line which gives the smallest SUM is then our regression line!  
The value of this SUM is called: __Sum of Squared of Residuals__ and we will label it as `SS_res`  

Now we can calculate the same SUM , but this time for the **average line** :

![r sq 2](images/r-squared_02.png)

so we are going to calculate : `SUM (y_i - y_avg)^2` and we are going to call it `SS_tot`

and then now the __R-squared__ is going to be defined like this:

![img.png](images/r-squared_03.png)

Your __goal__ and what's you try to do with your regression is to fit a line to minimize the `SS_res` (Sum of Residuals) and so bringing the value **R-squared** as close as possible to `1`

In other word the **R-squared** tells how good is your Regression-Line comparing the the Average-line.
___

## 8.2. Adjusted R-Squared Intuition
There's a Problem with **R-squared** :  
If you are going to add a new independent variable to your dataset, the `SS_res` is always going to get better (i.e. the minimum of the Sum decreases). Or in worst case scenario it's going to be the same if the coefficient of this new independent variable is going to be zero. So **R-squared will never decrease**

The problem is that if this new independent variable is not going to give more information about the __dependent variable__ y, it stills improve the **R-squared** .  

For example imagine the **Salary** as `y` and the **Years of experience** as `x_1` and then we are going to add the **Last Digit of Employee's Mobile Number** as `x_2` . As you see the `x_2` could have NOTHING to do with `y` and it's not going to improve the Prediction but still it's going to decrease the `SS_res` and hence increases the `R-sqaured`.  

The reason is that the coefficient of the `x_2` is **NOT going to be zero** because there is always a **randomly slight correlation** between the `x_2` and `y`

So there's a better way to calculate the R , which is called **Adjusted R-sqaured**

![img.png](images/r-sqaured_03.png)

which is `1 - (SS_res / SS_tot ) * ( n - 1 ) / ( n - p - 1 )`  

* `p` : number of regressors (number of features / independent variables)
* `n` : sample size

The `p` is like a **Penalization Factor** . That means it's going to penalize you if the added independent variable is NOT going to help your prediction.

We are going to use Adjusted R-Squared in this course to see if our __model is robust enough or not__.
___

___

# 9. Selecting Best Regression Model in Python

## 9.1. Preparation of the Regression Code Templates (Evaluate Regression Model)
How should we select the best model for our dataset? The simple answer is "Try every model. and then select the model which have __the best performance result__. The performance result is __measured by coefficient R-squared__."

`Data.csv` is quiet classic. but it is kind of a real world example with many datasets.  
This data set is from **UCI Machine Learning Repository** which you can take a look at. There are a lot dataset to practice.  
The independent variables in this data set gives the `PE` (dependent variable) , which is an Energy Output.  
Four features are:  
* `AT` = Ambient Temprature
* `V` = Exhaust Vacuum
* `AP` = Ambient pressure
* `RH` = Relative Humiditiy

We evaluate the performance of regression with:  
`from sklearn.metrics import r2_score`
___

## 9.2. The Ultimate Demo of the Powerful Regression Code Template in Action
So the winner model is the __Random Forest Regression__ with a score of more than `0.96`

So you should try different models and then compare the R-Squared value to find the best model for your dataset.
___
