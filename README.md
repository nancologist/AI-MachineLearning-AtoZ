# Machine Learning A-Z: Hands-On Python & R In Data Science

Install packages: `pip3 install $pkg && pip3 freeze > requirements.txt`

# 0. Welcome to the course
## 0.2. Meet your instructors
Hi there,

Hope you are enjoying the course so far!

Not so long ago Hadelin and I did an interview on the SDS Podcast. This is the best place to start if you would like to learn more about his background... and a bit about me too if this is your first course with me :)

Link: 

http://www.superdatascience.com/2

Some of the things you will learn in this podcast:
What is Machine Learning
Mastering Data Science through online courses
What are Recommender Systems
Million dollar question: R vs Python (vs Julia)
What Grand project Hadelin and I are currently working on
Plus you will get an overview of:
Regressions
Classifications
Clustering
Association rule learning
Reinforcement learning
Deep learning


See you in class!

Sincerely,

Kirill Eremenko
___

## 0.3. Learning Path
Hey Data Scientist,

SuperDataScience is bringing you a new learning experience. We know how difficult it is to carve out a career track so we’re introducing the [Machine Learning Skill Track](https://sdsclub.com/learning-paths/machine-learning-track/) to guarantee your way to success.

This Skill Track is a perfect fit if you:
* Struggle to determine the skills you need to succeed in this field,
* Are unsure which courses are right for you,
* Desire to arrange your learning curve efficiently and on your schedule.  

Built to deliver streamlined on-the-job success, the Machine Learning Skill Track provides structured curriculums and discounted courses for in-demand Machine Learning skills.

After completion, Skill Track students will walk away with the required Machine Learning skills and a complete portfolio of work to showcase in competitive job interviews.

Enter this Skill Track to start saving time and money on your Machine Learning journey today.
___

## 0.4. ML vs DL vs AI - What's the difference?
Dear Students,

Kick things off for the course by downloading a helpful cheat sheet: ‘ML vs DL vs AI — What’s the Difference?’ This download addresses one of the most popular questions we hear from students, and hopefully, it clarifies a few lingering questions for you too:

/articles/OU2-Difference-Between-ML-DL-AI.pdf

Enjoy ML!
___

## 0.5. Regression Types
Dear Students,

Watch a great educational video that speaks about the difference between simple linear regression, multiple linear regression, and polynomial regression. Save this resource, or come back and revisit this lecture once you start working on some of the regression models in the course. Please find the video [here](https://www.dropbox.com/s/py3ns8ltraoexfi/Youtube%20-%20Regression%20-%20Video%20Exp%20%232.mov?dl=0).

Enjoy ML!
___

## 0.6. Why Machin Learning is the future?
* For the Dawn of Time until 2005 the human race have created 130 EXABYTES!
* until 2010 that number was : 1,200 EXABYTES
* until 2015 : 7,900 EXABYTES
* estimated for 2020 : 40,900 EXABYTES

Maching Learning can help to use this huge Data more and better.
____

## 0.7. Important notes, tips & tricks for this course
Dear students,

We want you to have the best learning experience during the whole journey of this course. Therefore, please find just below some important notes to understand, as well as tips & tricks to take this course in the best conditions:

1. The practical activities of this course will be done in both Python and R. However, it is absolutely not required to do these practical cases in both programming languages. This course was designed so that people can learn Machine Learning whether they use Python or R in their company, their own business or any specific project. Hence everybody can get comfortable applying Machine Learning on their favorite programming language. But definitely you are not supposed to learn the two. You can do it if you want or if you need it for your work, but usually one of them is sufficient.

2. If you wish to learn both programming languages, just be prepared that there are going to be some repetitions. This is normal, the reasons for this is that there are a lot of similarities between Python and R and of course the practical cases solved in this course are the same for these two programming languages. However, just bear in mind that repetitions are not that bad: you will learn a lot of concepts and techniques in this course and the fact that we repeat the same elements from one programming language to another will help these concepts stick better into your head.

3. The video lectures of this course were recorded at a certain pace so that everybody can adjust the speed to learn at their own rhythm. To adjust the speed you simply need to click the following "Speed" button at the bottom left corner of the video lecture:


    If you find the pace too slow you can increase the speed to 1.25x, 1.5x or even 2x.  
    If you find the pace too fast you can decrease the speed to 0.75x or 0.5x.

4. If there is anything unclear or if you have any question during a Lecture, please keep in mind that there is a very high chance another student asked for some clarifications on the exact same element you need. You can easily find the answer to your question by browsing the Q&A of the specific Lecture you are watching. To browse the Q&A of the Lecture you simply need to click "Browse Q&A" at the bottom of the Lecture:Once you click "Browse Q&A" you will see all the questions and answers related to this Lecture on the right side of the screen:As you can see the questions have titles so you will easily find the same question you are wondering about. If you don't find your question you can of course ask a new question in the Q&A of the Lecture you are in.

We will add more tips & tricks based on our observations of students feedbacks. In the meantime we wish you the best learning experience.

Enjoy Machine Learning!

Kirill & Hadelin
___

## 0.8. This PDF resource will hepl you a lot!
Dear students,

We just made this Latex [PDF](/articles/Machine_Learning_A_Z_Q_A) for you which will considerably help you during your journey in this course. It contains in the first pages the whole structure of the course for you to visualize it clearly, and then 50 pages containing the answers to all the most frequently asked and most important questions, section by section.

In the Table of contents, the sections of the courses are hyperlinked, so you can very easily navigate to the section you want. For example, let's say you are in Part 2 - Regression, in the Multiple Linear Regression section, and you have a question on the Intuition Lectures. Well you just need to go to the Table of contents in the PDF, then go to Part 2 and then click "2.2.1 Multiple Linear Regression Intuition". And this will redirect you to the section you are looking for in the PDF. You may not find your question exactly, but the questions and answers that you will find will definitely bring you more clarity.

This PDF was written in Latex, which means it is super clean. We provided cleaned code snippets, nice charts, and all the mathematical equations to explain some concepts are written clearly.

This PDF will constantly be updated. As soon as we see new important and relevant questions that are repeatedly asked, we will add them in the PDF. Just make sure to re-download it in the course from time to time.

We hope this will help you and make you stronger in Machine Learning, keep up the great work!

Kirill & Hadelin

[PDF](/articles/Machine_Learning_A_Z_Q_A)
___

## 0.9. GET ALL THE CODES AND DATASETS HERE:
Hello my friends,

Please find below the link to the folder containing all the Python codes, R codes, and datasets of this course:

https://drive.google.com/drive/folders/1OFNnrHRZPZ3unWdErjLHod8Ibv2FfG1d?usp=sharing

This folder contains the Python files in .ipynb format, which is the format used to code in Python on Jupyter Notebook or Google Colaboratory.

__Important Note 1:__ Please download this folder on your machine in order to get all the files, especially the datasets which we will have to upload when training the Machine Learning models. In the practical tutorials you will be guided step by step on how to navigate this folder and start coding in Python & R.

__Important Note 2:__ (Python coders only): In order to open the Python files of this folder with Google Colaboratory, you need to have a Gmail account and sign in to that account. This will automatically open this folder on your Google Drive, which will indeed allow you to open the Python files with Google Colaboratory. Then once you open the file, it will be in read-only mode, so in order to code inside you just have to go to File, and then click "Save a copy in Drive...". This will create a copy of this file on your drive, inside which you will be able to code in Python.

Important Note 3 (Python coders only): If you don't want to code on Google Colaboratory and prefer to code on another Python IDE like Jupyter Notebook or Spyder in Anaconda, you can find all the .ipynb files and the .py files in the folder attached at the bottom of this article.

I can't wait to see you in the practical lectures.

Until then, enjoy Machine Learning!

Hadelin
___


## 0.10. Presentation of the ML A-Z folder, Colaboratory, Jupyter Notebook and Spyder
empty
___

## 0.11. Installing R and R Studio
1. https://cran.r-project.org/

2. Download and Install R

3. www.rstudio.com (IDE for R) (For PyCharm: https://www.jetbrains.com/help/pycharm/r-plugin-support.html#get-started)

4. Download and install RStudio
___

## 0.12. Some Additional Resources
Hey Data Scientist,

Congrats on enrolling in the Machine Learning A-Z course!

In order to ease in to this amazing field, we've selected a great episode you can listen to on your commute, at breakfast or wherever. 

Click here to get started: https://www.superdatascience.com/sds-041-inspiring-journey-totally-different-background-data-science/

Enjoy!
___

## 0.13. FAQBot!
Hello Students!

As an additional resource, we are working on deploying our FAQBot (Mango) to help answer any frequently asked - FAQ related questions for the course. You will see Mango (our bot) in the Q&A to help provide access to FAQ based questions and information quickly.

If you have a chance, please feel free test it out using the link mentioned below as we continue training and testing. To be clear, this bot is in early-stage development. Please be aware of any inaccuracy or error while we continue to improve it.

*Mango is still undergoing training, testing, and revisions.

Hi everyone and welcome to our new chatbot! We are hoping to use this chatbot to help answer some of your questions related to the course, or any general course related information to help continue learning. When interacting with the chatbot, please remember that this is a model in training and we expect to have quite a few iterations, development and testing phases to help it learn. Also, please be as clear as possible when asking questions with Mango.

You can ask questions such as:

How can I obtain the certificate of completion?

How to create a virtual environment?

What is the best choice for a programming language in ML or AI?

How to choose the number of hidden layers in a Neural Network?

Can you explain logistic regression?

Can you explain a KNN?

Should I use Python, R or both?

Where can I get the files for ML A-Z?



But please feel free to ask other course related questions so that Mango can learn. *Please keep them course related.

In addition, some other great resources for further information or assistance to help debug any error include the recommended readings, http://stackoverflow.com/, and https://datascience.stackexchange.com/. Our TA’s will do their best to help answer any questions but as DS/ML/AI engineers, becoming comfortable debugging, searching resources and documentation online will become a critical skill for success.

Lastly, please be aware of any errors, or unexpected outcomes. As Data Scientists we consistently need to test, develop and improve our models. We also want to thank you for helping to test our bot, and we hope it helps answer any questions that you might have!

To test Mango directly:

Mango:

https://www.superdatascience.com/pages/welcome-to-faqbot
___

## 0.14. Your Shortcut To Becoming A Better Data Scientist!
Hi Students!

Did you know that by participating in the Q&A it can help:

Make connections with peers within the domain

Reinforce course information

Gain useful insight related to course content

Lead to more course content completed

Improve and obtain better results from specific models

Have your model or customization featured in the course

One main way to help become a better Data Scientist is by joining the discussions. For this, we want to mention the importance of our community of students here. We continuously see and have had students post helpful information, impressive updates and customizations to algorithms that improved results, and have witnessed connections continuously being made in the Q&A.

We have even featured some students' contributions in the course itself! Imagine being able to mention in an interview, or with an employer that your model was featured in a top course in the Data Science or Artificial Intelligence domain?

This leads us to our main point of adding this which is to help emphasize the importance of self learning for this domain. We can easily show you the benefits of helping others (for those of you who are interested, please see here: https://www.mentalfloss.com/article/71964/7-scientific-benefits-helping-others but, in Machine Learning, Artificial Intelligence, Data Science, and any software or programming related career, it will be a required and critical skill to be able to search and solve problems as they develop. This can include any type of common bugs from programs or issues that may pop up due. As libraries, languages and IDE’s are consistently updated, you may run into a bug every so often. This is just part of the nature of the domain that we are in.

Due to this, we want to highlight the importance of self learning and accessing resources such as Google, StackOverflow (or any of the related StackExchanges), or even discussing questions amongst your fellow peers, is a phenomenal way to learn. For those of you who work in the domain, running into problems and solving them is a core method of learning. Personally, answering and debugging questions in the course has helped me become a better AI Engineer/Data Scientist and developer overall.

In addition, if you would like to practice the information from the course, reinforce concepts, and discuss ideas, discussing and answering questions of your peers is another important method to help learn. This can lead to making connections, study groups and habits, and overall improving your course experience by assisting others. Research has also shown that collaboration and participation can lead to increased completion of the material in the course.

To recap, participating in the Q&A in the course can help you with:

Form study habits

Reinforce course information and gain useful insight

Make connections with peers

Improve your overall course learning experience

Altruism

Purpose

*While participating in the Q&A please abide by Udemy’s Terms and Conditions, and be courteous to all participants. We are all on different stages of learning in the course, and every question is a great opportunity to learn!

We greatly appreciate all of the feedback we continuously receive and keep up the awesome work!
___


# 1. Data Preprocessing
## 1.0. Module Introduction
In each section we first start with Python and then do it with the R.
___

## 1.1. Getting Started
[Data.csv](chapter1_data_preprocessing/py/data/Data.csv) : This file here is like a Retail Company which analysis "Which client purchased one of their products, so these the rows (observations) i this dataset correspond to the different customers of this employee, and their infos... and the last column is about if they bought the Product or not."  

You can see that in this there are some empty cells, which makes this data source more realistic.
___

## 1.2. Importing the Libraries (Python)
For Machine Learning in Python we always need at least these three libraries:

* __Numpy__ : To work with arrays and do mathematical operations
* __matplotlib.pyplot__ : Which allows you to create charts
* __pandas__ : which allows you to import dataset and create easily matrices and vectors.

### Installation
So we should install these libraries which we need into our Virtual Env.:
* ``pip install numpy``
* ``pip install matplotlib``
* ``pip install pandas``
___

## 1.3. Importing Dataset (Python)
Data.csv : This file here is like a Retail Company which analysis "Which client purchased one of their products, so these the rows (observations) i this dataset correspond to the different customers of this employee, and their infos... and the last column is about if they bought the Product or not."

``pandas.read_csv()``: This creates a Data Frame from a .csv file.

### Next Steps
After importing Dataset and store it as a Data Frame we need to do these as the next steps:
1. Creating Matrix of Features.
2. Dependent Variable, a Vector.

### Important Principle in Machine Learning
_IN ANY DATASET WHICH YOU ARE GOING TO TRAIN A MACHINE LEARNING MODEL YOU HAVE THE ENTITIES WHICH ARE THE FEATURES AND THE DEPENDENT VARIABLE (two above steps)._

### Features (Independent Variables) - INPUT VALUES
Features are the columns, which are the independent informations (here: Country, Age, Salary), with them you are going to PREDICT the DEPENDENT VARIABLE (here: "Purchased") [Mori: For the future!].

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html  
``.ilock[ROWS, COLUMNS]`` : To locate indexes  (iloc stands for locate indexes)
``.iloc[:, :-1]`` : This takes all the Rows and also all the columns of the dataset, EXCEPT the last column (which is going to be predicted)

### Dependent Variable - TARGET VALUE (y)
This is usually the last column of the dataset (here: PURCHASED)! Because as you may guess, this company will PREDICT some future customers are going to buy the same product based on these informations.
___

## 1.4. For Python learners, summary of Object-oriented programming: classes & objects
Hello students,

For those of you interested in following the Python tutorials of this course, here is a short summary of what you need to know in Object-oriented programming. In the Python tutorials, I will be talking about classes, objects and methods. Please find below a clear explanation of what these concepts are:

A class is the model of something we want to build. For example, if we make a house construction plan that gathers the instructions on how to build a house, then this construction plan is the class.

An object is an instance of the class. So if we take that same example of the house construction plan, then an object is simply a house. A house (the object) that was built by following the instructions of the construction plan (the class).
And therefore there can be many objects of the same class, because we can build many houses from the construction plan.

A method is a tool we can use on the object to complete a specific action. So in this same example, a tool can be to open the main door of the house if a guest is coming. A method can also be seen as a function that is applied onto the object, takes some inputs (that were defined in the class) and returns some output.

Hope this helps you get the intuition of Object-oriented programming, don't hesitate to ask for more explanations in the Q&A if anything is unclear.

Kind regards,

Hadelin
___

## 1.5. Taking care of Missing Data
There are some missing data, which is normal in Machine Learning! If we look at the dataset we see that the Salary in Row 4 and the Age in the Raw 6 are missing.

You can not leave it like that, because it will cause error by training the model, therefore you must handle them. There are actually several ways to handle them:

1. The 1st way is just to IGNORE those Observations which hav missing data and deleting them. And that would be OK if you have a LARGE dataset so if you have for example 1% Missing data, you know that removing 1% of observations won't change much the LEARNING QUALITY of your MODEL. But sometimes you have a lot of missing data and therefore you must handle them the right way.

2. The 2nd way is to REPLACING the missing data by THE AVERAGE OF ALL THE VALUES IN THAT COLUMN (FEATURE), in which the data is missing.

3. Other ways could be to replace the missing value with the Median of that Column (Feature) or with the maximum frequent value in that column.

### Our Goals
* We want to replace the missing salary by the average of all the salaries, this is a CLASSIC WAY of handling missing data.

### Package ``Scikit Learn``
This is one of the important packages by the Data Preprocessing and it has very good tools for that. We will use it a lot in this course.

``pip install scikit-learn``

### How we handle it?
The class from the package Scikit-Learn is called ``SimpleImputer`` , we are actually going to first import that class, then we will create an Instance of that class, this object will allow us to exactly replace the missing salary, by the average of salaries

* `numpy.nan` : this respresents all the missing values in a CSV  
https://numpy.org/doc/stable/reference/constants.html#numpy.nan

* `strategy='mean'` : Replace the missing_values with the average

* ``fit(NUMERICAL_VALUES)`` It looks for the missing value and also calculate the replacement.
___

## 1.6. Categorical Data
What are categorical variables and how to encode categorical data, which is illustrated in Python by `LabelEncoder` and `OneHotEncoder` class from `sklearn.preprocessing` library, and in R the factor function to transform categorical data into numerical variables.

### 1.6.0. Label Encoder vs. One Hot Encoder (More)
So in the first column (Country) we have some countries which can't be understood by our ML Model. So we should make them number, we could have do that with Label Encoder which gives for example to French, Germany, Spain these numbers: 1,2,3  

Now the Problem is that our MODEL is going to misunderstand this, because it's going to compare these values! But we know that the Countries can not be compared like that! So what can help us here is ``One Hot Encoder``! This makes the string data numeric without allowing them to be compared!

(Mori: One-Hot-Encoder make a Unity Matrix (`n * n`) out of the categorical values. Where `n` is going to be the number of values)

IN OUR CASE the OneHotEncoder splits the Country-Column into THREE columns, because we have also 3 countries (If we had 5 Countries, the Country-Col would have been splitted into 5 columns).

THE ONE-HOT-ENCODER CREATES ``BINARY VECTORS`` FOR EACH COUNTRY.

### 1.6.1. Encoding Independent Variable (Features - X)
#### ``sklearn.compose.ColumnTransformer``
* Instructor about Line-32 to Line-34 : We have to enter two arguments:
    1. ``transfomers`` : with that we specify what kind of transformation we are going to do and which indexes of column we want to transform
    2. ``remainder='passthrough''`` : which specifies we actually want to keep the columns which won't get this transformation, meaning "Age" and "Salary" untouched!
    
    3. More details: ``tansformer=[(KIND_OF_TRANSFORMATION, TYPE_OF_ENCODING, COLUMNS_TO_BE_APPLIED)]``
    
    4. `fit_transform()` : It does the both FITTING and TRANSFORMING at once! (This was not possible by ``imputer``, so there we have first used ``.fit()`` and then ``.transform()`` in LINE-24 and LINE-28)
    
    5. LINE-38 : We use ``numpy.array()`` because ``ColumnTransformer()`` does not do that for us. And it should be a Numpy-Array, otherwise our Model can not train with it!
    
    6. _You now know to apply the One-Hot-Encoding when you have several categories in the matrix of features, but also you can do a simple Label-Encoding when you have two classes which you can directly encode to 0 and 1, in other word BINARY OUTCOME._

* https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html

* This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated TO FORM A SINGLE FEATURE SPACE. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.

* Parameter "transformers" : list of tuples  
List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.

* Parameter "remainder" , also called ESTIMATOR.

#### ``sklearn.preprocessing.OneHotEncoder``
* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder

* Encode categorical features as a one-hot numeric array.

* The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter)
___

### 1.6.2. Encoding Dependent Variable (y)
#### ``sklearn.preprocessing.LabelEncoder``
* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.htm  

* This transformer should be used to encode target values, i.e. ``y`` and not the input ``X``.

* It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.

(Mori:
    * Why is it ok to use LabelEncoder for a Binary Column and not OneHotEncoder?
    
    * Are binary values (True & False) comparable?! In my mind they are two independent states

    * If we had ONLY two values in our categorical columen (for example: category = country , values: France, Germany) would it have been ok to use LabelEncoder instead of OneHotEncoder? Would it have lead to a false training for our Model?
    
    * Is this not ok if we use OneHotEncoder on Dependent Columns?
)
ANSWER: https://en.wikipedia.org/wiki/Binary_data#In_statistics
___

## 1.7. Feature Scaling
### WHAT - Definition
__Feature Scaling is a technique that will put all your features in the SAME RANGE.__    

If we look at our ``Data.csv`` we can clearly see that the values of the Age-feature are NOT in the same RANGE as the values of the Salary-feature the Age-range is from like 0 to 100 and the Salary-Range is from 0 to 100 thousand.

Now we want to put the Age-Range and the Salary-Range in a same range using FEATURE SCALING technique.


### WHY - should we apply Feature Scaling?
For some of the ML Models (not all of them) if your different Features have a huge difference in range of their values, this can cause a __BIAS IN THE CORRELATIONS COMPUTATIONS__.  
In another word, the features that have higher values compared to the other ones will DOMINATE the other features so that these other features may NOT BE CONSIDERED in the Correlation Computation.

So depends on our Model sometimes we need to apply the Feature Scaling and sometimes it is not necessary, because the Model automatically can detect this issue and they fix this with ADAPTING the COEFFICIENTS (For example you see that with the Linear Regression. Linear Regression has some coefficients for each of the Features, so the Features with super HIGH VALUES will get a very LOW COEFFICIENTS. But for other regressions like Logistic Regressions or also the ML Model in R, we should apply the Feature Scaling)

### HOW - Feature Scaling Methods
* Standardisation : taking each value of the feature and subtract it by the Mean and then divided by the Standard Deviation. This puts all the values in range of usually between -3 and +3.

* Normalisation : In this we subtract the values of Feature by the Minimum value of the Feature and then divided by the Range (Max - Min). This will put all the values in the Feature between 0 and 1.

![feature scaling](./images/feature_scaling.png)

#### Standardisation vs. Normalisation
_Hadelin: I have had tons of experience with both and I did not observe much difference in the final accuracy and result between these two techniques._

``StandardScaler()`` keeps the X as numpy.array so we don't need to apply ``numpy.array()`` in LINE-48. 
___

## 1.8. Splitting Dataset

### 1.8.1 Splitting the Dataset into the Training and Test Set (Python)
In Machine Learning we split our data to a Training-Set and a Test-Set.  
You know that this is about the machine which is going to learn something to make predictions.

Imagine your machine learn to much on a dataset. Then we are not sure if its performance is great on a new set with a slightly different corrolations.

So we should always keep a part of data for the Test!

The performance of the Machine should not be that much different on the Test-Set comparing to Training-Set, so then we can conclude that this Model can understand Correlations (And he did not learn it by heart!) and so it can adapt the new sets of data in new situations.

BETWEEN 20% TO 30% OF DATA IS A GOOD CHOICE FOR THE TEST-PART!

### How Machine Learns Now?
Now the Machine Learning Model is going to find a CORRELATION between the X_train and y_train and with this Correlation it can predict a new_y for a new_X! or we can test it the quality of its Prediction with the X_test and y_test

___
### WARNING - Update (Part 1-8)
WARNING - Update
Dear students,

in the following tutorial, the first line of code we will type will be:

from sklearn.cross_validation import train_test_split 

However the "cross_validation" name is now deprecated and was replaced by "model_selection" inside the new anaconda versions.

Therefore you might get a warning or even an error if you run this line of code above.

To avoid this, you just need to replace:

``from sklearn.cross_validation import train_test_split`` 

by
``from sklearn.model_selection import train_test_split`` 
___


# 2. Regression
Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.

In this part, you will understand and learn how to implement the following Machine Learning Regression models:

* Simple Linear Regression
* Multiple Linear Regression
* Polynomial Regression
* Support Vector for Regression (SVR)
* Decision Tree Classification
* Random Forest Classification

## 2.1. Simple Linear Regression
### 2.1.1. Simple Linear Regression Intuition - Step 1
The linear equation and the name of its components:
![linear_equation](./images/linear_regression_01.png)

So we are going to look at an Example for Linear Regression: In the following we want to know homw the ``salary`` of employees in a company depends on their ``experience``.

![linear_regression_example](./images/linear_regression_02.png)

#### 2.1.1.1. Explaining The Above Diagram 
So now let's look at the simple in your regression because it's the easiest one to discuss. It's very pretty straightforward you can visualize it quite well.

So here we got the y and x axis. Let's look at that specific example where we have EXPERIENCE and SALARY. So experience is going to be our horizontal axis. Salary is all vertical axis and we want to understand how people's salary depends on their experience.

WELL WHAT WE DO IN REGRESSION IS WE DON'T JUST COME UP WITH A THEORY WE LOOK AT THE EVIDENCE WE'LL LOOK AT THE LIVE HARD FACTS SO HERE ARE SOME OBSERVATIONS WE'VE HAD.

So in a certain company this is how salaries are distributed among people who have different levels of experience and what a regression does.

So that's a formula for aggression. In our case it'll change to salary equals be zero plus ``b_1`` Times EXPERIENCE.

AND WHAT THAT ESSENTIALLY IT MEANS IS JUST PUTTING A LINE THROUGH YOUR CHART THAT BEST FITS THIS DATA and we'll talk about best fitting in the next tutorial when we're talking about ordinary squares.

But for now this is the chart. This is the line that best fits as Darren even looks like it right.
___

#### 2.1.1.2. Coefficient ``b_0``
For now let's focus on the coefficients and the caffeine and the constant.

So what does the constant mean here. Well the that actually means the point where the line crosses the vertical axis and let's say it's $30000.

What does that mean. Well it means that when when experience is zero. So when as you say on the horizontal axis when experience is at zero in the formula on the right you can see that the second part ``b_1`` Times experience becomes zero so salary equals zero.

That means that salary will equal to $30000 when a person has no experience so soon somebody is know fresh from University and joins this company. Most likely they will have a salary about $30000.
___

#### 2.1.1.3. Coefficient ``b_1``
Now what is ``b_1``, ``b_1`` IS THE SLOPE OF THE LINE. 

AND SO THE STEEPER THE LINE THE MORE YOU GET MORE MONEY YOU GET PER EXTRA YEAR OF EXPERIENCE.

Let's look at this. In this particular example let's say somebody went from I don't know maybe four to five years of ``experience``. So then to understand how his salary increase you have to project this onto the line and then project that onto the salary access and you can see that here for one of your experience the person will get AN EXTRA TEN THOUSAND DOLLARS ON TOP OF HIS SALARY.

So if the coefficient ``b_1`` is less, then the slope will be less and that means the salary increase will be less per every year of experience. If the slope is greater then that means the experience will yield more increase in salary and that's pretty much it.
___

That's how a simple your regression works. So the core goal here is that we're not just drawing a line theoretically that we can we came up with

SOME HOW WE'RE ACTUALLY USING OBSERVATIONS THAT WE HAVE TO FIND THE BEST FITTING LINE AND WHAT BEST FITTING LINE IS WE'LL TALK ABOUT THAT IN THE NEXT TUTORIAL.
___

### 2.1.2. Simple Linear Regression Intuition - Step 2

![linear_regression_HOW_works](./images/linear_regression_03.png)

#### 2.1.2.1 How Find Best Fitting Line
HOW THE LINEAR REGRESSION BEING A TREND LINE THAT BEST FITS YOUR DATA.

Today we'll find out how to find the best fitting light or in fact how the simple linear regression finds that line for you.

So here's our simple your aggression. The same chart salary versus experience. We've got these red dots which represent the actual observations that we have in our data and we've got the TREND LINE WHICH REPRESENTS THE BEST FITTING LINE OR THE SIMPLE LINEAR REGRESSION MODEL.

So now let's draw some vertical lines from the actual observations to the model. And let's look at one of the specific examples to understand what we're talking about here. 

So here you can see that the Red Cross is where that person is sitting at in terms of salary so let's say this person with 10 years of experience is earning $100000.

__-Interpretation-__  
WELL THE MODEL LINE (THE BLACK LINE), IT ACTUALLY TELLS US WHERE THAT PERSON SHOULD BE SITTING ACCORDING TO THE MODEL IN TERMS OF SALARY AND ACCORDING TO MODELS SHOULD BE A BIT LOWER. It should be somewhere without green crosses which is about maybe let's say thousand.

##### 2.1.2.2. Green And Red Pluses in Diagram:
So now the Red Cross is called ``y_i``. And that is the ACTUAL DURATION.

The Green Cross is called ``y_î`` (Y_i-hat)  is THE MODEL THE OBSERVATIONAL. or THE MODELED VALUE.

So basically with those that level of experience where would he be. Where does the model predict that he would be earning.

And so the green line therefore is the difference between what he's actually earning and what he should be earning.

So it should be what he's modeled to be earning. So therefore the green line will be the same regardless of what dependent variable you have whether it's salary or with it's grade school whatever. So it's the difference between the observed and the modeled for that level of independent variable.

#### 2.1.2.3. How Linear Regression Works!
Now to get this best fitting line what is done is you take the sum you take each one of those green lines are those distances (``y_i - y-î``) you square them and then you take some of those squares.

Once you have the sum of the squares for you got to find the MINIMUM of this ``SUM``!

So basically what a simple linear regression does is it draws lots and lots and lots of these lines. These trend lines all this is like a simplistic way of imagining the linear regression draws all these all possible trend trend lines (_Mori: Trend Lines are those vertical green lines!_) and counts the sum of those squares every single time.

And it store these SUMs somewhere in a temporary you know file or something like that and then 

IT FINDS THE MINIMUM ONE SO IT LOOKS FOR THE MINIMUM SUM OF SQUARES AND FINDS A LINE WHICH HAS THE SMALLEST SUM OF SQUARES POSSIBLE.

and that line will be the best fitting line and that is called the ordinary least squares method.

So that's how the simple linear regression works and look for you on the next tutorial.

___

### 2.1.3. Simple Linear Regression in Python - Importing and Splitting Data
#### 2.1.3.1. Data
* Every Row is corresponding to an EMPLOYEE (Observation).
* For each Employee we have two data : Years of Experience and Salary
___

#### 2.1.3.2. Goal
Building a simply linear regression model which will be trained to understand the correlation between the years of experience and the salary.
___

### 2.1.4. Simple Linear Regression in Python - Train Model
ERROR: ``X = dataset.iloc[:, 1].values`` had created a 1D array but for the ``linear_regression.fit()`` the ``X`` should be a 2D array. so i changed it to: ``X = dataset.iloc[:, 0:1].values``
___

### 2.1.5. Simple Linear Regression in Python - Predict Salaries
``y_pred`` is the vector of PREDICTIONS of the Dependent Variable. Here in our example this is the PREDICTED SALARY for all Observations in our TEST-SET.

Now we should compare the Prediction (``y_pred``) with ``y_test`` (y_test is the real data)
___

### 2.1.6. Py - Visualizing Data
* ``pyplot.show()`` To Notify Python that we are at the end of creating a plot and we want to show it! 

![salary_xp_trainingset](./chapter2_regression/chapter2-1_linear_regression/python/plots/salay_xp_trainingset_linear_reg.png)

FOR THE TEST-SET YOU CAN GET A SIMILAR PLOT LIKE ABOVE WITH RUNNING THE PYTHON CODE!

__IMPORTANT:__  
To plot the Test-Set , also the Scatter Points of Test-Set and the Linear Regression we should NOT change the LINE-68 and 69 in ``simple_linear_regression.R`` the ``training_set`` to ``test-set``.

BECAUSE OUR MODEL IS ALREADY TRAINED ON TRAINING-SET, IF WE REPLACE THE TRAINING-SET WITH TEST-SET IN LINE-68 AND 69 IN ``simple_linear_regression.R`` WE OBTAIN THE SAME RESULT! 

INDEED IF WE REPLACE HERE THE TRAINING-SET WITH TEST-SET WE WOULD JUST BUILD SOME NEW POINTS OF THIS REGRESSION LINE CORRESPONDING TO PREDICTIONS, BECAUSE WHEN WE TRAINED OUR SIMPLE LINEAR REGRESSOR ON THE TRAINING SET WE OBTAINED ONE UNIQUE MODEL EQUATION.
___
___

### 2.1.7. R - Importing & Splitting Data
___

### 2.1.8. R - Fit Simple Linear Regression to Training Set
#### lm(FORMULA, TRAINING_SET) - Linear Models
``Salary ~ YearsExpriens`` : This means that Salary is proportion to Years of Experience.
___

#### Summary()
If you run now ``summary(linear_regresor)`` you get a result which contains the following:
````
Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)      25811.8     2707.4   9.534 2.86e-09 ***
YearsExperience   9426.6      431.8  21.834  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
````

Not only it tells the value of your coefficients in the simple linear equation. But it also tells you the STATISTICAL SIGNIFICANCES of your coefficients.  
Here we see 3 Stars (***) for the YearsExperience and this means that this Dependent Value is HIGHLY STATISTICALLY SIGNIFICANT.

The reason here that the YearsExperience so so Highly Statistically Significant is that it has a STRONG RELATIONSHIP with Salary.

#### Pr(>|t|) - "P-value"
It is another indicator of Statistically Significance, because the LOWER the Pr() is, the MORE Significant your Dependent Variable is going to be, that is the more impact your Independent Variable (X) has on your Dependent Variable (y).

A good threshold for the Pr() is ``5%``, which means that whenever we are below 5%, the Independent Variable (y) is highly statistically significant, and when we are over 5% that means this is less statistically significant. 

And here we can see that Pr() value for the ``YearsExperience`` (Independent Variable) is less than ``2e-16`` which means that this is very very small. That means that the ``YearsExperience`` is HIGHLY STATISTICALLY SIGNIFICANT and has a high effect on Dependent Variable ``Salary`` in the formula ``lm(formula = Salary ~ Experience, ....)`` 

___

### 2.1.9. R - Predicting Test-Set Result

___

### 2.1.10. R - Visualising Data (Scattered Points) and Prediction (LinReg)
* We need here ``ggplots`` package for R.

__IMPORTANT:__  
To plot the Test-Set , also the Scatter Points of Test-Set and the Linear Regression we should NOT change the LINE-68 and 69 in ``simple_linear_regression.R`` the ``training_set`` to ``test-set``.

BECAUSE OUR MODEL IS ALREADY TRAINED ON TRAINING-SET, IF WE REPLACE THE TRAINING-SET WITH TEST-SET IN LINE-68 AND 69 IN ``simple_linear_regression.R`` WE OBTAIN THE SAME RESULT! 

INDEED IF WE REPLACE HERE THE TRAINING-SET WITH TEST-SET WE WOULD JUST BUILD SOME NEW POINTS OF THIS REGRESSION LINE CORRESPONDING TO PREDICTIONS, BECAUSE WHEN WE TRAINED OUR SIMPLE LINEAR REGRESSOR ON THE TRAINING SET WE OBTAINED ONE UNIQUE MODEL EQUATION.
___


## 2.2. Multiple Linear Regression
### 2.2.1. Explaining Data - 50 Startups
This is about 50 companies the table has 5 Columns which shows how much these companies spend in R&D (Research & Development), Administration and Marketing and the how much Profit (Dependent Value) they gained in that year.

There's a Venture Capitalist Fund (VCF) that has hired you as a data scientist. You should find out:

* Which types of companies it is more interested in INVESTING and their main criteria is PROFIT.

* So you have to create a model which tells you about Profit based on these independent data.

* They are not just looking at the Highest Profit. But what they are looking for is:
    1. They want to understand IN WHICH STATE A COMPANY PERFORMS BETTER? 
    
    2. Or which companies perform better ignoring the state , in which the companies are, (as they were in teh same state.) So for example they want to know IS THE COMPANY WHICH INVEST MORE IN MARKETING BETTER? OR THE COMPANY WHICH INVESTS IN THE ADMIN? OR THE ONE WHICH INVEST MORE IN R&D?
    
    3. They want also to understand 
    
#### Example
Based on your model that you'll create they will have a They'll set up a set of guidelines for their own venture capitalist fund. 

And they'll be for example say this: 

_OK SO WE ARE MORE INTERESTED IN COMPANIES WHICH WORK OR OPERATE IN NEW YORK AND THAT HAVE A VERY LOW ADMINISTRATION SPEND AND A VERY HIGH R&D SPEND, MEANS THE R&D SPEND HAS TO BE MUCH HIGHER THAN ADMINISTRATION MARKING SPEND_

__So basically you helping them create a model based off of this sample that will allow them to assess where and in which into which companies they want to invest to achieve their goal of maximizing profit.__
    
    
___

### 2.2.2. Explaining Multiple Linear Regression
![multi linear regression](./images/multiple_lin_reg_01.png)

Multiple linear regression the same thing like simple linear regression but many variables so b0 is constant and then many combinations or many pairs of B and X and it can go up to quite a lot DEPENDS ON HOW MANY REGRESSORS YOU HAVE HERE.

Once again you've got a dependent variable y, so something you're explaining but this time the difference is that you think there might be a few causation.

So in the case of salary for instance it could be how much years of experience you've done how many other courses you've done, how much you know how much time you spend at work or things like that or how valuable how much money you make for the company in terms of a Student and his grades if your dependent variable is ``y =`` What grade does a student get. and the independent variables could be how much the student has studied for the exam maybe how much he has slept before the exam how many lectures he has attended throughout the course and the things like that.

The constant (b0) is still there and also you've got the coefficients (b1, b2, ...).
___

### 2.2.3. Conditions for Using Multiple Linear Regression *
Attention! Your data should have the following conditions so that using the Linear Regression for the Model would be suitable:

1. LINEARITY
2. HOMOSCEDASTICITY
3. MULTIVARIATE NORMALITY
4. INDEPENDENCE OF ERRORS
5. LACK OF MULTICOLLINEARITY

(\* Mori: I should look at these subjects later...)
___

### 2.2.4. Dummy Variables & Dummy Variables Trap
![multi linear regression](./images/dummy_variable.png)

Like in the last chapter we ONE-HOT-ENCODE the categorical variables. But ATTENTION! If we have two categorical variable we put just ONE of them in the equation (for example New York = D1) , so D1 can be 0 or 1.

SO WE DON'T NEED A DUMMY VAR ALSO FOR CALIFORNIA BECAUSE WHEN IT'S NOT NEW YORK SO THIS CHANGES THE EQUATION TO ANOTHER STATE. SO THERE'S NO NEED FOR A D2!

#### WHY N-1 DUMMY VARIABLES (N = Number of Categorical Variables in a Column)
So if you add a ``b5 * D2`` you are DUPLICATING THE ``CALIFORNIA`` because ``D2 = 1 - D1``.

#### Multicollinearity
The phenomenon where one or several independent variables in a linear regression predict another is called __MULTICOLLINEARITY__.  
As a result of this effect. The Model can not distinguish the effects of D1 from the effects of D2. And this is called DUMMY VARIABLE TRAP
___

### 2.2.5. What is the P-Value?
#### Definitions
* __Null Hypothesis:__  
    The hypothesis that there is no relationship between the experimental variable(s) and the observed results.
    
* __Degrees of Freedom:__  
    The equation for degrees of freedom is Degrees of freedom = n-1, where "n" is the number of categories or variables being analyzed in your experiment.

* __Significance Level:__  
    By convention, scientists usually set the significance value for their experiments at 0.05, or 5 percent. This means that experimental results that meet this significance level have, at most, a 5% chance of being reproduced in a random sampling process.
    
* __P Values:__  
    * Usually, if the P value of a data set is below a certain pre-determined amount (like, for instance, 0.05), scientists will reject the "null hypothesis" of their experiment - in other words, they'll rule out the hypothesis that the variables of their experiment had no meaningful effect on the results.
    
    * If your P VALUE is LOWER than your SIGNIFICANCE VALUE, CONGRATULATIONS! - you've shown that your experimental results would be highly unlikely to occur if there was no real connection between the variables you manipulated and the effect you observed. BUT IF YOUR P VALUE IS HIGHER THAN YOUR SIGNIFICANCE VALUE, YOU CAN'T CONFIDENTLY MAKE THAT CLAIM.
    
MORI: PLEASE READ THE PDF FILE IN ``articles/`` and the Step 1 to Step 7 , BUT YOU CAN FIRST USE THE ABOVE DEFINITIONS FOR A BETTER UNDERSTANDING OF THE SUBJECTS.
___

#### 2.2.6. Methods of Building a Modal
_Mori: Look at the according PDF._

### PDF - Page 2: Methods
We have 5 methods to building models. The methods 2.Backward Elimination, 3.Forward Selection and 4.Bidirectional Elimination are called Stepwise Regression (Their algorithm has loop which checks one or two conditions.)

#### PDF - Page 3: "All-in" Cases
Sometimes you MUST use all variables that you got!

#### PDF - Page 4: Backward Elimination
* In step 3 , if the PREDICTOR (variable) have a P-Value greater than Significance Level then this PREDICTOR should be removed. 

* Step 5 : This is actually step 2 without the removed PREDICTOR.

* If the fitted Model hit the ``P-value < Significane-Level`` then your MODEL IS READY!

#### PDF - Page 5: Forward Selection
* In step 2 you fit the y (Dep. Var.) individually with each X (Indep. Var.)

* Step 3: Add the other Indep. Vars to this Simple Regression (the one with the lowest P-value) and check that again , and after this step add one more

* ATTENTION : Do this until the ``P > SL`` so this means your Previous Model was the READY MODEL. THE CURRENT MODEL IS NOT THE RIGHT ONE, BECAUSE THIS HAS BROKEN THE RULE!

__IN THIS CHAPTER WE ARE GOING TO USE ``BACKWARD-ELIMINATION``!__
___

### 2.2.7. Add Data - 50 StartUps

1. Always first  you need to have a to check your data with watching it (If it's not too long), so you can check that you have no missing data. 

2. IF THE DATA IS TOO LANG, then apply the DATA-PREPROCESSING Tools that handle the missing data.

3. Check if any feature is categorical and apply ONE HOT ENCODING there!
___

### 2.2.8 Importing, Encoding and Splitting Data
* ATTENTION: Here we don't have to apply Feature-Scaling. Because in the Equation of Multiple Linear Regression you have coefficients (b_i) which are multiplied to indep. variables (x_i) so that it does not matter that some feature have higher values than others, because the COEFFICIENTS COMPENSATE TO PUT EVERYTHING ON THE SAME SCALE.

* _Do we need to check the assumptions of linear regression?_  
    The answer is absolutely NO!  
    Because I will explain at the end of this part that whenever you have a new dataset and you want to experiment with some Machine Learning Models to figure out which one leads to the highest accuracy. 
    
    Well even if your dataset doesn't have linear relationships you can still try a Multiple Linear Regression on it.  
    And if you know your dataset doesn't have linear relationships well then your multiple linear regression will just perform poorly and therefore it will get an accuracy lower than the accuracy of your other models so you are going not to choose the multiple linear regression.  
    But you don't have to check the multiple in our regression assumptions it will just be a waste of time.
    
    We will learn how to apply different regressions on the model and find the best one very fast.
___

### 2.2.9. Train Model (Multiple Linear Regression)
* Do we need to do something to avoid the Dummy Variable Trap?  

    The answer is no. Because the Mutli-Lin-Reg Class which we are going to avoid this trap! (So it removes one of the three one-hot-encoded columns)
    
* Do we need to work on the Features (Indep. Vars) like applying the Backward Elimination technique in order to select the features which have the lowest P-values (statistically significant)?

    The answer is again NO! The Class of Multi Linear Regression will take care of that and finds the Most Statistically Significant Indep Vars).
    
* Here we use the same Class as in the Simple Linear Regression and it detects that here we have multiple Ind. Vars (Xs)

* Plot? Here we can not plot like the last section (Simple Linear Regression) because here we have multiple lines, but what we are going to plot is a COMPARING BETWEEN REAL AND PREDICTED PROFITS of 10 datasets in Test set (50 x 0.2 = 10 Datasets).
___

### 2.2.10. Multiple Linear Regression in Python - Step 4
### 2.2.11. Multiple Linear Regression in Python - Automatic Backward Elimination
___
___
___

### 2.2.12. Multiple Linear Regression in R - Step 1
### 2.2.13. Multiple Linear Regression in R - Step 2
### 2.2.14. Multiple Linear Regression in R - Step 3
### 2.2.15. Multiple Linear Regression in R - Backward Elimination - HOMEWORK !
### 2.2.18. Multiple Linear Regression in R - Backward Elimination - Homework Solution
### 2.2.19. Multiple Linear Regression in R - Automatic Backward Elimination
___

## 2.3. Polynomial Regression
...
___

## 2.4. Support Vector Regression (SVR)
...
___

## 2.5. Decision Tree Regression
...
___

## 2.6. Random Forest Regression
...
___

## 2.7. Evaluating Regression Models Performance
...
___








